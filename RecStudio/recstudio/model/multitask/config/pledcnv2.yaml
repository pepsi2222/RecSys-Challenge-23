model:
  num_levels: 1
  specific_experts_per_task: 2
  num_shared_experts: 1
  # expert_mlp_layer: [128, 128]
  # expert_activation: relu
  # expert_dropout: 0.1
  expert_num_layers: 2

  gate_mlp_layer: [256, 256, ]
  gate_activation: relu
  gate_dropout: 0.1

  tower_mlp_layer: [256, 256, ]
  tower_activation: relu
  tower_dropout: 0.1
  tower_batch_norm: False

train:
  weight_decay: 1e-5