model:
  embed_dim: 40

  top_mlp_layer: [128, 128]
  top_activation: relu
  top_dropout: 0
  top_batch_norm: False

  bottom_mlp_layer: [128, 128]
  bottom_activation: relu
  bottom_dropout: 0
  bottom_batch_norm: False

  reduction_ratio: 3
  excitation_activation: relu


train:
  weights: [1.0, 20.0]
  weight_decay: 1e-6
