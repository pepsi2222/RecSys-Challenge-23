model:
  tower_mlp_layer: [128, 64]
  tower_activation: relu
  tower_dropout: 0
  tower_batch_norm: False
  alpha: 0.6

train:
  weight_decay: 1e-5
