model:
  num_levels: 2
  specific_experts_per_task: 4
  num_shared_experts: 3
  expert_mlp_layer: [256, 256]
  expert_activation: relu
  expert_dropout: 0.5

  gate_mlp_layer: [128, ]
  gate_activation: relu
  gate_dropout: 0.1

  tower_mlp_layer: [128,]
  tower_activation: relu
  tower_dropout: 0.1
  tower_batch_norm: False

  batch_norm: True
  mlp_layer: [128, 128]
  activation: relu
  dropout: 0.3

  reduction_ratio: 3
  excitation_activation: relu

  embed_dim: 40

  weights: [1.0, 20.0]


train:
  weight_decay: 1e-6