model:
  log_hidden_size: 128
  mlp_layer: [400, 400, 400]
  activation: relu
  dropout: 0.5
  embed_dim: 10

  ensemble: True
  ensemble_mlp_layer: [256, 64]
  ensemble_activation: relu
  ensemble_dropout: 0.5

train:
  learning_rate: 1e-3
  weight_decay: 1e-6

