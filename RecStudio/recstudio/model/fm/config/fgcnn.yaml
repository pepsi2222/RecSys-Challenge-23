model:
  channels: [3, 4, 5]
  heights: [16, 16, 16]
  recombine_channels: [5, 6, 7]
  pooling_sizes: [3, 1, 1]

  mlp_layer: [256]
  activation: relu
  dropout: 0.5
  batch_norm: True