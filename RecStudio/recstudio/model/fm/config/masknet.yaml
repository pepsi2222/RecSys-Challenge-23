model:
  parallel: False
  num_blocks: 3
  block_dim: 50
  reduction_ratio: 1
  hidden_layer_norm: False
  mlp_layer: [512, 128]
  activation: relu
  dropout: 0

train:
  learning_rate: 1e-3
  scheduler: exponential
  weight_decay: 1e-6