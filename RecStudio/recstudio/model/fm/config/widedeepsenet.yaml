model:
  batch_norm: True
  embed_dim: 50
  mlp_layer: [256, 256, 256]
  activation: relu
  dropout: 0.3

  reduction_ratio: 3
  excitation_activation: relu

  embed_dim: 64