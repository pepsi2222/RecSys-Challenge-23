model:
  product_type: inner
  mlp_layer: [512,256]
  activation: relu
  dropout: 0.3
  batch_norm: False
  stack_dim: 2
