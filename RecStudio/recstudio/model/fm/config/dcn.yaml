model:
  embed_dim: 10
  mlp_layer: [512, 512, 256]
  activation: relu
  num_layers: 6
  dropout: 0.5
  batch_norm: True

train:
  learning_rate: 1e-3
  weight_decay: 1e-6