model:
  embed_dim: 10
  mlp_layer: [128, 128, 128]
  dropout: 0.3
  batch_norm: True
  activation: sigmoid

  reduction_ratio: 3
  excitation_activation: relu

  embed_dim: 64