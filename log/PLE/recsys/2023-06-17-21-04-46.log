[2023-06-17 21:04:46] INFO Log saved in /data2/home/xingmei/RecSys23/log/PLE/recsys/2023-06-17-21-04-46.log.
[2023-06-17 21:04:46] INFO Global seed set to 2022
[2023-06-17 21:04:46] INFO Load dataset from cache.
[2023-06-17 21:04:48] INFO 
Dataset Info: 

======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================
interaction information: 
field        f_1          f_2          f_3          f_4          f_5          f_6          f_8          f_9          f_10         f_11         f_12         f_13         f_14         f_15         f_16         f_17         f_18         f_19         f_20         f_21         f_22         f_23         f_24         f_25         f_26         f_30         f_31         f_32         f_33         f_34         f_35         f_36         f_37         f_38         f_39         f_40         f_41         f_42         f_43         f_44         f_45         f_46         f_47         f_48         f_49         f_50         f_51         f_52         f_53         f_54         f_55         f_56         f_57         f_58         f_59         f_60         f_61         f_62         f_63         f_64         f_65         f_66         f_67         f_68         f_69         f_70         f_71         f_72         f_73         f_74         f_75         f_76         f_77         f_78         f_79         is_clicked   is_installed 
type         float        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        float        token        token        token        token        token        token        token        token        token        token        token        token        token        token        float        float        token        token        token        token        float        float        float        float        float        float        float        token        token        token        token        token        token        token        token        token        float        float        
##           -            140          6            639          7            5235         7            8            4            25           27           332          20           5855         13           50           925          20           58           36           27           5            5            4            3            3            3            5            3            3            3            3            3            3            3            3            3            8883         -            22           24           12           28           28           21           35           1829         172          103          213          390          221          517          -            -            403          826          1397         408          -            -            -            -            -            -            -            5            12           9            5            32           9            5            14           8            -            -            
======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================
Total Interactions: 3646825
======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================
f_1=StandardScaler()
f_43=StandardScaler()
f_58=StandardScaler()
f_59=StandardScaler()
f_64=MinMaxScaler()
f_65=StandardScaler()
f_66=StandardScaler()
f_67=StandardScaler()
f_68=StandardScaler()
f_69=StandardScaler()
f_70=StandardScaler()
[2023-06-17 21:04:48] INFO 
Model Config: 

data:
	binarized_rating_thres=None
	fm_eval=False
	neg_count=0
	sampler=None
	shuffle=False
	split_mode=entry
	split_ratio=[3387880, 97972, 160973]
	fmeval=True
	low_rating_thres=None
eval:
	batch_size=4096
	cutoff=[5, 10, 20]
	val_metrics=['logloss', 'auc', 'accuracy', 'f1', 'precision', 'recall']
	val_n_epoch=1
	test_metrics=['logloss', 'auc', 'accuracy', 'f1', 'precision', 'recall']
	topk=100
	save_path=./saved/
	binarized_prob_thres=0.5
	main_task=is_installed
model:
	embed_dim=64
	item_bias=False
	num_levels=2
	specific_experts_per_task=4
	num_shared_experts=3
	expert_mlp_layer=[128, 128]
	expert_activation=relu
	expert_dropout=0.1
	gate_mlp_layer=[128]
	gate_activation=relu
	gate_dropout=0.1
	tower_mlp_layer=[128]
	tower_activation=relu
	tower_dropout=0.1
	tower_batch_norm=False
train:
	accelerator=gpu
	ann=None
	batch_size=512
	early_stop_mode=min
	early_stop_patience=10
	epochs=1000
	gpu=[8]
	grad_clip_norm=None
	init_method=xavier_normal
	item_batch_size=1024
	learner=adam
	learning_rate=0.001
	num_threads=10
	sampling_method=none
	sampler=uniform
	negative_count=0
	excluding_hist=False
	scheduler=None
	seed=2022
	weight_decay=1e-05
	tensorboard_path=None
	weights=None
[2023-06-17 21:04:48] INFO save_dir:./saved/
[2023-06-17 21:04:48] INFO PLE(
  (loss_fn): BCEWithLogitLoss()
  (embedding): Embeddings(
    num_features=75, embed_dim=64, reduction=mean
    (embeddings): ModuleDict(
      (f_16): Embedding(13, 64, padding_idx=0)
      (f_15): Embedding(5855, 64, padding_idx=0)
      (f_54): Embedding(213, 64, padding_idx=0)
      (f_66): DenseEmbedding(
        embedding_dim=64, bias=False, batch_norm=True
        (batch_norm_layer): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (weight): Linear(in_features=1, out_features=64, bias=False)
      )
      (f_12): Embedding(27, 64, padding_idx=0)
      (f_60): Embedding(403, 64, padding_idx=0)
      (f_77): Embedding(5, 64, padding_idx=0)
      (f_56): Embedding(221, 64, padding_idx=0)
      (f_61): Embedding(826, 64, padding_idx=0)
      (f_41): Embedding(3, 64, padding_idx=0)
      (f_35): Embedding(3, 64, padding_idx=0)
      (f_38): Embedding(3, 64, padding_idx=0)
      (f_30): Embedding(3, 64, padding_idx=0)
      (f_23): Embedding(5, 64, padding_idx=0)
      (f_65): DenseEmbedding(
        embedding_dim=64, bias=False, batch_norm=True
        (batch_norm_layer): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (weight): Linear(in_features=1, out_features=64, bias=False)
      )
      (f_74): Embedding(5, 64, padding_idx=0)
      (f_6): Embedding(5235, 64, padding_idx=0)
      (f_18): Embedding(925, 64, padding_idx=0)
      (f_33): Embedding(3, 64, padding_idx=0)
      (f_58): DenseEmbedding(
        embedding_dim=64, bias=False, batch_norm=True
        (batch_norm_layer): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (weight): Linear(in_features=1, out_features=64, bias=False)
      )
      (f_21): Embedding(36, 64, padding_idx=0)
      (f_49): Embedding(21, 64, padding_idx=0)
      (f_31): Embedding(3, 64, padding_idx=0)
      (f_53): Embedding(103, 64, padding_idx=0)
      (f_5): Embedding(7, 64, padding_idx=0)
      (f_17): Embedding(50, 64, padding_idx=0)
      (f_26): Embedding(3, 64, padding_idx=0)
      (f_1): DenseEmbedding(
        embedding_dim=64, bias=False, batch_norm=True
        (batch_norm_layer): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (weight): Linear(in_features=1, out_features=64, bias=False)
      )
      (f_47): Embedding(28, 64, padding_idx=0)
      (f_8): Embedding(7, 64, padding_idx=0)
      (f_45): Embedding(24, 64, padding_idx=0)
      (f_69): DenseEmbedding(
        embedding_dim=64, bias=False, batch_norm=True
        (batch_norm_layer): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (weight): Linear(in_features=1, out_features=64, bias=False)
      )
      (f_37): Embedding(3, 64, padding_idx=0)
      (f_70): DenseEmbedding(
        embedding_dim=64, bias=False, batch_norm=True
        (batch_norm_layer): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (weight): Linear(in_features=1, out_features=64, bias=False)
      )
      (f_52): Embedding(172, 64, padding_idx=0)
      (f_76): Embedding(9, 64, padding_idx=0)
      (f_55): Embedding(390, 64, padding_idx=0)
      (f_44): Embedding(22, 64, padding_idx=0)
      (f_71): Embedding(5, 64, padding_idx=0)
      (f_14): Embedding(20, 64, padding_idx=0)
      (f_63): Embedding(408, 64, padding_idx=0)
      (f_46): Embedding(12, 64, padding_idx=0)
      (f_42): Embedding(8883, 64, padding_idx=0)
      (f_22): Embedding(27, 64, padding_idx=0)
      (f_24): Embedding(5, 64, padding_idx=0)
      (f_3): Embedding(6, 64, padding_idx=0)
      (f_36): Embedding(3, 64, padding_idx=0)
      (f_32): Embedding(5, 64, padding_idx=0)
      (f_51): Embedding(1829, 64, padding_idx=0)
      (f_9): Embedding(8, 64, padding_idx=0)
      (f_59): DenseEmbedding(
        embedding_dim=64, bias=False, batch_norm=True
        (batch_norm_layer): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (weight): Linear(in_features=1, out_features=64, bias=False)
      )
      (f_34): Embedding(3, 64, padding_idx=0)
      (f_11): Embedding(25, 64, padding_idx=0)
      (f_10): Embedding(4, 64, padding_idx=0)
      (f_68): DenseEmbedding(
        embedding_dim=64, bias=False, batch_norm=True
        (batch_norm_layer): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (weight): Linear(in_features=1, out_features=64, bias=False)
      )
      (f_39): Embedding(3, 64, padding_idx=0)
      (f_64): DenseEmbedding(
        embedding_dim=64, bias=False, batch_norm=True
        (batch_norm_layer): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (weight): Linear(in_features=1, out_features=64, bias=False)
      )
      (f_67): DenseEmbedding(
        embedding_dim=64, bias=False, batch_norm=True
        (batch_norm_layer): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (weight): Linear(in_features=1, out_features=64, bias=False)
      )
      (f_75): Embedding(32, 64, padding_idx=0)
      (f_19): Embedding(20, 64, padding_idx=0)
      (f_73): Embedding(9, 64, padding_idx=0)
      (f_20): Embedding(58, 64, padding_idx=0)
      (f_25): Embedding(4, 64, padding_idx=0)
      (f_72): Embedding(12, 64, padding_idx=0)
      (f_78): Embedding(14, 64, padding_idx=0)
      (f_48): Embedding(28, 64, padding_idx=0)
      (f_2): Embedding(140, 64, padding_idx=0)
      (f_57): Embedding(517, 64, padding_idx=0)
      (f_43): DenseEmbedding(
        embedding_dim=64, bias=False, batch_norm=True
        (batch_norm_layer): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (weight): Linear(in_features=1, out_features=64, bias=False)
      )
      (f_4): Embedding(639, 64, padding_idx=0)
      (f_62): Embedding(1397, 64, padding_idx=0)
      (f_79): Embedding(8, 64, padding_idx=0)
      (f_50): Embedding(35, 64, padding_idx=0)
      (f_13): Embedding(332, 64, padding_idx=0)
      (f_40): Embedding(3, 64, padding_idx=0)
    )
  )
  (extraction_layers): Sequential(
    (0): ExtractionLayer(
      specific_experts_per_task=4, num_task=2, num_shared_experts=3
      (specific_experts): ModuleList(
        (0-1): 2 x ModuleList(
          (0-3): 4 x MLPModule(
            (model): Sequential(
              (0): Dropout(p=0.1, inplace=False)
              (1): Linear(in_features=4800, out_features=128, bias=True)
              (2): ReLU()
              (3): Dropout(p=0.1, inplace=False)
              (4): Linear(in_features=128, out_features=128, bias=True)
              (5): ReLU()
            )
          )
        )
      )
      (shared_experts): ModuleList(
        (0-2): 3 x MLPModule(
          (model): Sequential(
            (0): Dropout(p=0.1, inplace=False)
            (1): Linear(in_features=4800, out_features=128, bias=True)
            (2): ReLU()
            (3): Dropout(p=0.1, inplace=False)
            (4): Linear(in_features=128, out_features=128, bias=True)
            (5): ReLU()
          )
        )
      )
      (gates): ModuleList(
        (0-1): 2 x MLPModule(
          (model): Sequential(
            (0): Dropout(p=0.1, inplace=False)
            (1): Linear(in_features=4800, out_features=128, bias=True)
            (2): ReLU()
            (3): Dropout(p=0.1, inplace=False)
            (4): Linear(in_features=128, out_features=7, bias=True)
            (5): ReLU()
            (6): Softmax(dim=-1)
          )
        )
      )
      (shared_gates): MLPModule(
        (model): Sequential(
          (0): Dropout(p=0.1, inplace=False)
          (1): Linear(in_features=4800, out_features=128, bias=True)
          (2): ReLU()
          (3): Dropout(p=0.1, inplace=False)
          (4): Linear(in_features=128, out_features=11, bias=True)
          (5): ReLU()
          (6): Softmax(dim=-1)
        )
      )
    )
    (1): ExtractionLayer(
      specific_experts_per_task=4, num_task=2, num_shared_experts=3
      (specific_experts): ModuleList(
        (0-1): 2 x ModuleList(
          (0-3): 4 x MLPModule(
            (model): Sequential(
              (0): Dropout(p=0.1, inplace=False)
              (1): Linear(in_features=128, out_features=128, bias=True)
              (2): ReLU()
              (3): Dropout(p=0.1, inplace=False)
              (4): Linear(in_features=128, out_features=128, bias=True)
              (5): ReLU()
            )
          )
        )
      )
      (shared_experts): ModuleList(
        (0-2): 3 x MLPModule(
          (model): Sequential(
            (0): Dropout(p=0.1, inplace=False)
            (1): Linear(in_features=128, out_features=128, bias=True)
            (2): ReLU()
            (3): Dropout(p=0.1, inplace=False)
            (4): Linear(in_features=128, out_features=128, bias=True)
            (5): ReLU()
          )
        )
      )
      (gates): ModuleList(
        (0-1): 2 x MLPModule(
          (model): Sequential(
            (0): Dropout(p=0.1, inplace=False)
            (1): Linear(in_features=128, out_features=128, bias=True)
            (2): ReLU()
            (3): Dropout(p=0.1, inplace=False)
            (4): Linear(in_features=128, out_features=7, bias=True)
            (5): ReLU()
            (6): Softmax(dim=-1)
          )
        )
      )
    )
  )
  (towers): ModuleDict(
    (is_clicked): MLPModule(
      (model): Sequential(
        (0): Dropout(p=0.1, inplace=False)
        (1): Linear(in_features=128, out_features=128, bias=True)
        (2): ReLU()
        (3): Dropout(p=0.1, inplace=False)
        (4): Linear(in_features=128, out_features=1, bias=True)
      )
    )
    (is_installed): MLPModule(
      (model): Sequential(
        (0): Dropout(p=0.1, inplace=False)
        (1): Linear(in_features=128, out_features=128, bias=True)
        (2): ReLU()
        (3): Dropout(p=0.1, inplace=False)
        (4): Linear(in_features=128, out_features=1, bias=True)
      )
    )
  )
)
[2023-06-17 21:04:48] INFO GPU id [8] are selected.
[2023-06-17 21:09:05] INFO Training: Epoch=  0 [train_loss_0=0.2750 is_clicked logloss=0.4927 accuracy=0.7891 f1=0.6490 precision=0.8178 recall=0.5380 auc=0.7933 train_loss_0=0.2812 is_installed logloss=0.3986 accuracy=0.8213 f1=0.2934 precision=0.5667 recall=0.1980 auc=0.7947 train_loss_0=0.2688]
[2023-06-17 21:09:05] INFO Train time: 255.84295s. Valid time: 0.59104s. GPU RAM: 0.56/10.76 GB
[2023-06-17 21:09:05] INFO logloss improved. Best value: 0.3986
[2023-06-17 21:13:23] INFO Training: Epoch=  1 [train_loss_0=0.2632 is_clicked logloss=0.5232 accuracy=0.7836 f1=0.6406 precision=0.8043 recall=0.5324 auc=0.7881 train_loss_0=0.2679 is_installed logloss=0.3922 accuracy=0.8170 f1=0.3063 precision=0.5290 recall=0.2157 auc=0.8113 train_loss_0=0.2585]
[2023-06-17 21:13:23] INFO Train time: 256.55668s. Valid time: 0.94588s. GPU RAM: 0.60/10.76 GB
[2023-06-17 21:13:24] INFO logloss improved. Best value: 0.3922
[2023-06-17 21:17:35] INFO Training: Epoch=  2 [train_loss_0=0.2614 is_clicked logloss=0.6116 accuracy=0.7373 f1=0.5922 precision=0.6772 recall=0.5262 auc=0.7659 train_loss_0=0.2659 is_installed logloss=0.4098 accuracy=0.8116 f1=0.2245 precision=0.4915 recall=0.1456 auc=0.7917 train_loss_0=0.2569]
[2023-06-17 21:17:35] INFO Train time: 249.98151s. Valid time: 0.83696s. GPU RAM: 0.62/10.76 GB
[2023-06-17 21:21:51] INFO Training: Epoch=  3 [train_loss_0=0.2607 is_clicked logloss=0.7132 accuracy=0.7288 f1=0.5916 precision=0.6514 recall=0.5419 auc=0.7622 train_loss_0=0.2652 is_installed logloss=0.4190 accuracy=0.8102 f1=0.2560 precision=0.4825 recall=0.1743 auc=0.7834 train_loss_0=0.2562]
[2023-06-17 21:21:51] INFO Train time: 254.68191s. Valid time: 0.59198s. GPU RAM: 0.62/10.76 GB
[2023-06-17 21:26:11] INFO Training: Epoch=  4 [train_loss_0=0.2603 is_clicked logloss=0.5905 accuracy=0.7315 f1=0.5957 precision=0.6558 recall=0.5458 auc=0.7792 train_loss_0=0.2649 is_installed logloss=0.3976 accuracy=0.8073 f1=0.2513 precision=0.4620 recall=0.1727 auc=0.8007 train_loss_0=0.2557]
[2023-06-17 21:26:11] INFO Train time: 259.01624s. Valid time: 0.64649s. GPU RAM: 0.62/10.76 GB
[2023-06-17 21:30:27] INFO Training: Epoch=  5 [train_loss_0=0.2599 is_clicked logloss=0.6724 accuracy=0.7286 f1=0.5920 precision=0.6504 recall=0.5433 auc=0.7512 train_loss_0=0.2643 is_installed logloss=0.4121 accuracy=0.8070 f1=0.2384 precision=0.4577 recall=0.1612 auc=0.7847 train_loss_0=0.2554]
[2023-06-17 21:30:27] INFO Train time: 255.91984s. Valid time: 0.62516s. GPU RAM: 0.62/10.76 GB
[2023-06-17 21:35:00] INFO Training: Epoch=  6 [train_loss_0=0.2597 is_clicked logloss=0.7000 accuracy=0.7346 f1=0.6081 precision=0.6544 recall=0.5681 auc=0.7708 train_loss_0=0.2642 is_installed logloss=0.3952 accuracy=0.8118 f1=0.2950 precision=0.4952 recall=0.2102 auc=0.8047 train_loss_0=0.2552]
[2023-06-17 21:35:00] INFO Train time: 272.17431s. Valid time: 0.64383s. GPU RAM: 0.62/10.76 GB
[2023-06-17 21:39:26] INFO Training: Epoch=  7 [train_loss_0=0.2595 is_clicked logloss=0.6667 accuracy=0.7313 f1=0.6048 precision=0.6478 recall=0.5672 auc=0.7698 train_loss_0=0.2641 is_installed logloss=0.4082 accuracy=0.8075 f1=0.2762 precision=0.4676 recall=0.1961 auc=0.7845 train_loss_0=0.2550]
[2023-06-17 21:39:26] INFO Train time: 264.98580s. Valid time: 0.55441s. GPU RAM: 0.62/10.76 GB
[2023-06-17 21:43:37] INFO Training: Epoch=  8 [train_loss_0=0.2593 is_clicked logloss=0.6615 accuracy=0.7275 f1=0.5981 precision=0.6426 recall=0.5595 auc=0.7658 train_loss_0=0.2638 is_installed logloss=0.3970 accuracy=0.8080 f1=0.3246 precision=0.4761 recall=0.2463 auc=0.8015 train_loss_0=0.2548]
[2023-06-17 21:43:37] INFO Train time: 250.04560s. Valid time: 0.59473s. GPU RAM: 0.62/10.76 GB
[2023-06-17 21:47:49] INFO Training: Epoch=  9 [train_loss_0=0.2591 is_clicked logloss=0.6709 accuracy=0.7341 f1=0.6037 precision=0.6563 recall=0.5591 auc=0.7667 train_loss_0=0.2637 is_installed logloss=0.3888 accuracy=0.8148 f1=0.2529 precision=0.5183 recall=0.1674 auc=0.8125 train_loss_0=0.2546]
[2023-06-17 21:47:49] INFO Train time: 251.51460s. Valid time: 0.56927s. GPU RAM: 0.62/10.76 GB
[2023-06-17 21:47:50] INFO logloss improved. Best value: 0.3888
[2023-06-17 21:52:18] INFO Training: Epoch= 10 [train_loss_0=0.2590 is_clicked logloss=0.6875 accuracy=0.7317 f1=0.5933 precision=0.6583 recall=0.5402 auc=0.7640 train_loss_0=0.2636 is_installed logloss=0.4057 accuracy=0.8136 f1=0.2391 precision=0.5091 recall=0.1563 auc=0.7938 train_loss_0=0.2545]
[2023-06-17 21:52:18] INFO Train time: 266.43425s. Valid time: 0.68525s. GPU RAM: 0.62/10.76 GB
[2023-06-17 21:56:55] INFO Training: Epoch= 11 [train_loss_0=0.2590 is_clicked logloss=0.6405 accuracy=0.7296 f1=0.6013 precision=0.6458 recall=0.5627 auc=0.7681 train_loss_0=0.2634 is_installed logloss=0.4048 accuracy=0.8058 f1=0.2869 precision=0.4599 recall=0.2086 auc=0.7883 train_loss_0=0.2546]
[2023-06-17 21:56:55] INFO Train time: 276.13321s. Valid time: 0.62918s. GPU RAM: 0.62/10.76 GB
[2023-06-17 22:01:35] INFO Training: Epoch= 12 [train_loss_0=0.2589 is_clicked logloss=0.7202 accuracy=0.7168 f1=0.5905 precision=0.6204 recall=0.5634 auc=0.7404 train_loss_0=0.2634 is_installed logloss=0.4026 accuracy=0.8083 f1=0.2618 precision=0.4701 recall=0.1815 auc=0.7874 train_loss_0=0.2544]
[2023-06-17 22:01:35] INFO Train time: 279.91793s. Valid time: 0.63161s. GPU RAM: 0.62/10.76 GB
[2023-06-17 22:06:11] INFO Training: Epoch= 13 [train_loss_0=0.2587 is_clicked logloss=0.7140 accuracy=0.7098 f1=0.5701 precision=0.6158 recall=0.5308 auc=0.7513 train_loss_0=0.2632 is_installed logloss=0.4178 accuracy=0.8057 f1=0.2498 precision=0.4520 recall=0.1727 auc=0.7751 train_loss_0=0.2543]
[2023-06-17 22:06:11] INFO Train time: 275.16143s. Valid time: 0.60136s. GPU RAM: 0.62/10.76 GB
[2023-06-17 22:10:51] INFO Training: Epoch= 14 [train_loss_0=0.2587 is_clicked logloss=0.6996 accuracy=0.7283 f1=0.5914 precision=0.6499 recall=0.5427 auc=0.7534 train_loss_0=0.2632 is_installed logloss=0.4070 accuracy=0.8066 f1=0.2685 precision=0.4613 recall=0.1894 auc=0.7922 train_loss_0=0.2542]
[2023-06-17 22:10:51] INFO Train time: 279.32696s. Valid time: 0.64602s. GPU RAM: 0.62/10.76 GB
[2023-06-17 22:15:27] INFO Training: Epoch= 15 [train_loss_0=0.2587 is_clicked logloss=0.6966 accuracy=0.7184 f1=0.5975 precision=0.6199 recall=0.5768 auc=0.7612 train_loss_0=0.2631 is_installed logloss=0.4081 accuracy=0.8065 f1=0.3162 precision=0.4680 recall=0.2389 auc=0.7911 train_loss_0=0.2542]
[2023-06-17 22:15:27] INFO Train time: 274.90735s. Valid time: 0.64682s. GPU RAM: 0.62/10.76 GB
[2023-06-17 22:20:00] INFO Training: Epoch= 16 [train_loss_0=0.2586 is_clicked logloss=0.6926 accuracy=0.7240 f1=0.5955 precision=0.6351 recall=0.5606 auc=0.7555 train_loss_0=0.2630 is_installed logloss=0.4072 accuracy=0.8089 f1=0.2879 precision=0.4776 recall=0.2062 auc=0.7865 train_loss_0=0.2542]
[2023-06-17 22:20:00] INFO Train time: 272.49665s. Valid time: 0.60542s. GPU RAM: 0.62/10.76 GB
[2023-06-17 22:24:36] INFO Training: Epoch= 17 [train_loss_0=0.2585 is_clicked logloss=0.6999 accuracy=0.7280 f1=0.5885 precision=0.6517 recall=0.5366 auc=0.7572 train_loss_0=0.2631 is_installed logloss=0.4063 accuracy=0.8091 f1=0.2753 precision=0.4767 recall=0.1936 auc=0.7922 train_loss_0=0.2540]
[2023-06-17 22:24:36] INFO Train time: 275.20375s. Valid time: 0.62839s. GPU RAM: 0.62/10.76 GB
[2023-06-17 22:29:11] INFO Training: Epoch= 18 [train_loss_0=0.2584 is_clicked logloss=0.6853 accuracy=0.7301 f1=0.5885 precision=0.6576 recall=0.5327 auc=0.7716 train_loss_0=0.2630 is_installed logloss=0.4126 accuracy=0.8109 f1=0.2057 precision=0.4832 recall=0.1307 auc=0.7943 train_loss_0=0.2539]
[2023-06-17 22:29:11] INFO Train time: 273.76408s. Valid time: 0.62213s. GPU RAM: 0.62/10.76 GB
[2023-06-17 22:33:35] INFO Training: Epoch= 19 [train_loss_0=0.2584 is_clicked logloss=0.6536 accuracy=0.7346 f1=0.5986 precision=0.6627 recall=0.5459 auc=0.7773 train_loss_0=0.2629 is_installed logloss=0.4028 accuracy=0.8118 f1=0.3001 precision=0.4948 recall=0.2155 auc=0.7999 train_loss_0=0.2539]
[2023-06-17 22:33:35] INFO Train time: 263.46724s. Valid time: 0.62303s. GPU RAM: 0.62/10.76 GB
[2023-06-17 22:33:35] INFO Early stopped. Since the metric logloss haven't been improved for 10 epochs.
[2023-06-17 22:33:35] INFO The best score of logloss is 0.3888 on epoch 9
[2023-06-17 22:33:35] INFO Best model checkpoint saved in ./saved/PLE/recsys/2023-06-17-21-04-46.ckpt.
[2023-06-17 22:33:59] INFO Predictions saved in ./predictions/PLE/2023-06-17-22-33-35['is_clicked', 'is_installed'].csv
