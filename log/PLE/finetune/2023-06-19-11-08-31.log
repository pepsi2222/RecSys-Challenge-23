[2023-06-19 11:08:31] INFO Log saved in /data2/home/xingmei/RecSys23/log/PLE/finetune/2023-06-19-11-08-31.log.
[2023-06-19 11:08:31] INFO Global seed set to 2022
[2023-06-19 11:08:31] INFO Load dataset from cache.
[2023-06-19 11:08:32] INFO 
Dataset Info: 

======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================
interaction information: 
field        f_1          f_2          f_3          f_4          f_5          f_6          f_8          f_9          f_10         f_11         f_12         f_13         f_14         f_15         f_16         f_17         f_18         f_19         f_20         f_21         f_22         f_23         f_24         f_25         f_26         f_30         f_31         f_32         f_33         f_34         f_35         f_36         f_37         f_38         f_39         f_40         f_41         f_42         f_43         f_44         f_45         f_46         f_47         f_48         f_49         f_50         f_51         f_52         f_53         f_54         f_55         f_56         f_57         f_58         f_59         f_60         f_61         f_62         f_63         f_64         f_65         f_66         f_67         f_68         f_69         f_70         f_71         f_72         f_73         f_74         f_75         f_76         f_77         f_78         f_79         is_clicked   is_installed 
type         float        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        float        token        token        token        token        token        token        token        token        token        token        token        token        token        token        float        float        token        token        token        token        float        float        float        float        float        float        float        token        token        token        token        token        token        token        token        token        float        float        
##           -            136          6            631          7            4455         7            8            4            25           25           206          19           4277         11           42           543          20           56           35           25           5            5            4            3            3            3            5            3            3            3            3            3            3            3            3            3            6725         -            18           19           11           21           21           15           22           1725         133          85           172          306          182          415          -            -            232          584          1032         298          -            -            -            -            -            -            -            5            12           9            5            32           9            5            11           7            -            -            
======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================
Total Interactions: 1045756
======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================
f_1=StandardScaler()
f_43=StandardScaler()
f_58=StandardScaler()
f_59=StandardScaler()
f_64=MinMaxScaler()
f_65=StandardScaler()
f_66=StandardScaler()
f_67=StandardScaler()
f_68=StandardScaler()
f_69=StandardScaler()
f_70=StandardScaler()
[2023-06-19 11:08:32] INFO 
Model Config: 

data:
	binarized_rating_thres=None
	fm_eval=False
	neg_count=0
	sampler=None
	shuffle=False
	split_mode=entry
	split_ratio=[1016364, 29392, 0]
	fmeval=True
	low_rating_thres=None
eval:
	batch_size=4096
	cutoff=[5, 10, 20]
	val_metrics=['logloss', 'auc', 'accuracy', 'f1', 'precision', 'recall']
	val_n_epoch=1
	test_metrics=['logloss', 'auc', 'accuracy', 'f1', 'precision', 'recall']
	topk=100
	save_path=./saved/
	binarized_prob_thres=0.5
	main_task=is_installed
model:
	embed_dim=64
	item_bias=False
	num_levels=2
	specific_experts_per_task=4
	num_shared_experts=3
	expert_mlp_layer=[128, 128]
	expert_activation=relu
	expert_dropout=0.1
	gate_mlp_layer=[128]
	gate_activation=relu
	gate_dropout=0.1
	tower_mlp_layer=[128]
	tower_activation=relu
	tower_dropout=0.1
	tower_batch_norm=False
train:
	accelerator=gpu
	ann=None
	batch_size=512
	early_stop_mode=min
	early_stop_patience=10
	epochs=1000
	gpu=[0]
	grad_clip_norm=None
	init_method=xavier_normal
	item_batch_size=1024
	learner=adam
	learning_rate=0.001
	num_threads=10
	sampling_method=none
	sampler=uniform
	negative_count=0
	excluding_hist=False
	scheduler=None
	seed=2022
	weight_decay=1e-05
	tensorboard_path=None
	weights=[1.0, 10.0]
[2023-06-19 11:08:32] INFO save_dir:./saved/
[2023-06-19 11:08:32] INFO PLE(
  (loss_fn): BCEWithLogitLoss()
  (embedding): Embeddings(
    num_features=75, embed_dim=64, reduction=mean
    (embeddings): ModuleDict(
      (f_69): DenseEmbedding(
        embedding_dim=64, bias=False, batch_norm=True
        (batch_norm_layer): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (weight): Linear(in_features=1, out_features=64, bias=False)
      )
      (f_65): DenseEmbedding(
        embedding_dim=64, bias=False, batch_norm=True
        (batch_norm_layer): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (weight): Linear(in_features=1, out_features=64, bias=False)
      )
      (f_44): Embedding(18, 64, padding_idx=0)
      (f_64): DenseEmbedding(
        embedding_dim=64, bias=False, batch_norm=True
        (batch_norm_layer): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (weight): Linear(in_features=1, out_features=64, bias=False)
      )
      (f_43): DenseEmbedding(
        embedding_dim=64, bias=False, batch_norm=True
        (batch_norm_layer): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (weight): Linear(in_features=1, out_features=64, bias=False)
      )
      (f_9): Embedding(8, 64, padding_idx=0)
      (f_57): Embedding(415, 64, padding_idx=0)
      (f_48): Embedding(21, 64, padding_idx=0)
      (f_66): DenseEmbedding(
        embedding_dim=64, bias=False, batch_norm=True
        (batch_norm_layer): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (weight): Linear(in_features=1, out_features=64, bias=False)
      )
      (f_58): DenseEmbedding(
        embedding_dim=64, bias=False, batch_norm=True
        (batch_norm_layer): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (weight): Linear(in_features=1, out_features=64, bias=False)
      )
      (f_26): Embedding(3, 64, padding_idx=0)
      (f_18): Embedding(543, 64, padding_idx=0)
      (f_35): Embedding(3, 64, padding_idx=0)
      (f_67): DenseEmbedding(
        embedding_dim=64, bias=False, batch_norm=True
        (batch_norm_layer): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (weight): Linear(in_features=1, out_features=64, bias=False)
      )
      (f_41): Embedding(3, 64, padding_idx=0)
      (f_2): Embedding(136, 64, padding_idx=0)
      (f_68): DenseEmbedding(
        embedding_dim=64, bias=False, batch_norm=True
        (batch_norm_layer): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (weight): Linear(in_features=1, out_features=64, bias=False)
      )
      (f_70): DenseEmbedding(
        embedding_dim=64, bias=False, batch_norm=True
        (batch_norm_layer): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (weight): Linear(in_features=1, out_features=64, bias=False)
      )
      (f_38): Embedding(3, 64, padding_idx=0)
      (f_30): Embedding(3, 64, padding_idx=0)
      (f_21): Embedding(35, 64, padding_idx=0)
      (f_75): Embedding(32, 64, padding_idx=0)
      (f_31): Embedding(3, 64, padding_idx=0)
      (f_72): Embedding(12, 64, padding_idx=0)
      (f_14): Embedding(19, 64, padding_idx=0)
      (f_5): Embedding(7, 64, padding_idx=0)
      (f_51): Embedding(1725, 64, padding_idx=0)
      (f_20): Embedding(56, 64, padding_idx=0)
      (f_54): Embedding(172, 64, padding_idx=0)
      (f_56): Embedding(182, 64, padding_idx=0)
      (f_71): Embedding(5, 64, padding_idx=0)
      (f_17): Embedding(42, 64, padding_idx=0)
      (f_61): Embedding(584, 64, padding_idx=0)
      (f_12): Embedding(25, 64, padding_idx=0)
      (f_73): Embedding(9, 64, padding_idx=0)
      (f_3): Embedding(6, 64, padding_idx=0)
      (f_1): DenseEmbedding(
        embedding_dim=64, bias=False, batch_norm=True
        (batch_norm_layer): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (weight): Linear(in_features=1, out_features=64, bias=False)
      )
      (f_79): Embedding(7, 64, padding_idx=0)
      (f_25): Embedding(4, 64, padding_idx=0)
      (f_60): Embedding(232, 64, padding_idx=0)
      (f_53): Embedding(85, 64, padding_idx=0)
      (f_74): Embedding(5, 64, padding_idx=0)
      (f_50): Embedding(22, 64, padding_idx=0)
      (f_24): Embedding(5, 64, padding_idx=0)
      (f_46): Embedding(11, 64, padding_idx=0)
      (f_49): Embedding(15, 64, padding_idx=0)
      (f_8): Embedding(7, 64, padding_idx=0)
      (f_63): Embedding(298, 64, padding_idx=0)
      (f_52): Embedding(133, 64, padding_idx=0)
      (f_4): Embedding(631, 64, padding_idx=0)
      (f_45): Embedding(19, 64, padding_idx=0)
      (f_34): Embedding(3, 64, padding_idx=0)
      (f_62): Embedding(1032, 64, padding_idx=0)
      (f_55): Embedding(306, 64, padding_idx=0)
      (f_23): Embedding(5, 64, padding_idx=0)
      (f_76): Embedding(9, 64, padding_idx=0)
      (f_6): Embedding(4455, 64, padding_idx=0)
      (f_47): Embedding(21, 64, padding_idx=0)
      (f_77): Embedding(5, 64, padding_idx=0)
      (f_15): Embedding(4277, 64, padding_idx=0)
      (f_40): Embedding(3, 64, padding_idx=0)
      (f_59): DenseEmbedding(
        embedding_dim=64, bias=False, batch_norm=True
        (batch_norm_layer): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (weight): Linear(in_features=1, out_features=64, bias=False)
      )
      (f_36): Embedding(3, 64, padding_idx=0)
      (f_22): Embedding(25, 64, padding_idx=0)
      (f_37): Embedding(3, 64, padding_idx=0)
      (f_42): Embedding(6725, 64, padding_idx=0)
      (f_33): Embedding(3, 64, padding_idx=0)
      (f_11): Embedding(25, 64, padding_idx=0)
      (f_10): Embedding(4, 64, padding_idx=0)
      (f_39): Embedding(3, 64, padding_idx=0)
      (f_78): Embedding(11, 64, padding_idx=0)
      (f_32): Embedding(5, 64, padding_idx=0)
      (f_19): Embedding(20, 64, padding_idx=0)
      (f_16): Embedding(11, 64, padding_idx=0)
      (f_13): Embedding(206, 64, padding_idx=0)
    )
  )
  (extraction_layers): Sequential(
    (0): ExtractionLayer(
      specific_experts_per_task=4, num_task=2, num_shared_experts=3
      (specific_experts): ModuleList(
        (0-1): 2 x ModuleList(
          (0-3): 4 x MLPModule(
            (model): Sequential(
              (0): Dropout(p=0.1, inplace=False)
              (1): Linear(in_features=4800, out_features=128, bias=True)
              (2): ReLU()
              (3): Dropout(p=0.1, inplace=False)
              (4): Linear(in_features=128, out_features=128, bias=True)
              (5): ReLU()
            )
          )
        )
      )
      (shared_experts): ModuleList(
        (0-2): 3 x MLPModule(
          (model): Sequential(
            (0): Dropout(p=0.1, inplace=False)
            (1): Linear(in_features=4800, out_features=128, bias=True)
            (2): ReLU()
            (3): Dropout(p=0.1, inplace=False)
            (4): Linear(in_features=128, out_features=128, bias=True)
            (5): ReLU()
          )
        )
      )
      (gates): ModuleList(
        (0-1): 2 x MLPModule(
          (model): Sequential(
            (0): Dropout(p=0.1, inplace=False)
            (1): Linear(in_features=4800, out_features=128, bias=True)
            (2): ReLU()
            (3): Dropout(p=0.1, inplace=False)
            (4): Linear(in_features=128, out_features=7, bias=True)
            (5): ReLU()
            (6): Softmax(dim=-1)
          )
        )
      )
      (shared_gates): MLPModule(
        (model): Sequential(
          (0): Dropout(p=0.1, inplace=False)
          (1): Linear(in_features=4800, out_features=128, bias=True)
          (2): ReLU()
          (3): Dropout(p=0.1, inplace=False)
          (4): Linear(in_features=128, out_features=11, bias=True)
          (5): ReLU()
          (6): Softmax(dim=-1)
        )
      )
    )
    (1): ExtractionLayer(
      specific_experts_per_task=4, num_task=2, num_shared_experts=3
      (specific_experts): ModuleList(
        (0-1): 2 x ModuleList(
          (0-3): 4 x MLPModule(
            (model): Sequential(
              (0): Dropout(p=0.1, inplace=False)
              (1): Linear(in_features=128, out_features=128, bias=True)
              (2): ReLU()
              (3): Dropout(p=0.1, inplace=False)
              (4): Linear(in_features=128, out_features=128, bias=True)
              (5): ReLU()
            )
          )
        )
      )
      (shared_experts): ModuleList(
        (0-2): 3 x MLPModule(
          (model): Sequential(
            (0): Dropout(p=0.1, inplace=False)
            (1): Linear(in_features=128, out_features=128, bias=True)
            (2): ReLU()
            (3): Dropout(p=0.1, inplace=False)
            (4): Linear(in_features=128, out_features=128, bias=True)
            (5): ReLU()
          )
        )
      )
      (gates): ModuleList(
        (0-1): 2 x MLPModule(
          (model): Sequential(
            (0): Dropout(p=0.1, inplace=False)
            (1): Linear(in_features=128, out_features=128, bias=True)
            (2): ReLU()
            (3): Dropout(p=0.1, inplace=False)
            (4): Linear(in_features=128, out_features=7, bias=True)
            (5): ReLU()
            (6): Softmax(dim=-1)
          )
        )
      )
    )
  )
  (towers): ModuleDict(
    (is_clicked): MLPModule(
      (model): Sequential(
        (0): Dropout(p=0.1, inplace=False)
        (1): Linear(in_features=128, out_features=128, bias=True)
        (2): ReLU()
        (3): Dropout(p=0.1, inplace=False)
        (4): Linear(in_features=128, out_features=1, bias=True)
      )
    )
    (is_installed): MLPModule(
      (model): Sequential(
        (0): Dropout(p=0.1, inplace=False)
        (1): Linear(in_features=128, out_features=128, bias=True)
        (2): ReLU()
        (3): Dropout(p=0.1, inplace=False)
        (4): Linear(in_features=128, out_features=1, bias=True)
      )
    )
  )
)
[2023-06-19 11:08:32] INFO GPU id [0] are selected.
[2023-06-19 11:09:44] INFO Training: Epoch=  0 [train_loss_0=0.2823 is_clicked logloss=0.6260 accuracy=0.6359 f1=0.0000 precision=0.0000 recall=0.0000 auc=0.6899 train_loss_0=0.4603 is_installed logloss=0.3701 accuracy=0.8267 f1=0.2661 precision=0.6587 recall=0.1669 auc=0.8354 train_loss_0=0.2823]
[2023-06-19 11:09:44] INFO Train time: 70.60827s. Valid time: 0.18447s. GPU RAM: 0.55/10.76 GB
[2023-06-19 11:09:44] INFO logloss improved. Best value: 0.3701
[2023-06-19 11:10:53] INFO Training: Epoch=  1 [train_loss_0=0.2676 is_clicked logloss=0.6437 accuracy=0.6359 f1=0.0000 precision=0.0000 recall=0.0000 auc=0.6543 train_loss_0=0.4492 is_installed logloss=0.3757 accuracy=0.8263 f1=0.3128 precision=0.6168 recall=0.2098 auc=0.8224 train_loss_0=0.2676]
[2023-06-19 11:10:53] INFO Train time: 68.81550s. Valid time: 0.23458s. GPU RAM: 0.56/10.76 GB
[2023-06-19 11:12:03] INFO Training: Epoch=  2 [train_loss_0=0.2638 is_clicked logloss=0.6476 accuracy=0.6359 f1=0.0000 precision=0.0000 recall=0.0000 auc=0.6279 train_loss_0=0.4508 is_installed logloss=0.3875 accuracy=0.8152 f1=0.3230 precision=0.5225 recall=0.2343 auc=0.8080 train_loss_0=0.2638]
[2023-06-19 11:12:03] INFO Train time: 69.93970s. Valid time: 0.18021s. GPU RAM: 0.56/10.76 GB
[2023-06-19 11:13:14] INFO Training: Epoch=  3 [train_loss_0=0.2610 is_clicked logloss=0.6382 accuracy=0.6359 f1=0.0000 precision=0.0000 recall=0.0000 auc=0.6466 train_loss_0=0.4527 is_installed logloss=0.3771 accuracy=0.8189 f1=0.3395 precision=0.5435 recall=0.2472 auc=0.8222 train_loss_0=0.2609]
[2023-06-19 11:13:14] INFO Train time: 70.21299s. Valid time: 0.18738s. GPU RAM: 0.56/10.76 GB
[2023-06-19 11:14:24] INFO Training: Epoch=  4 [train_loss_0=0.2584 is_clicked logloss=0.6532 accuracy=0.6359 f1=0.0000 precision=0.0000 recall=0.0000 auc=0.6377 train_loss_0=0.4549 is_installed logloss=0.3904 accuracy=0.8207 f1=0.3018 precision=0.5677 recall=0.2058 auc=0.8141 train_loss_0=0.2583]
[2023-06-19 11:14:24] INFO Train time: 70.63257s. Valid time: 0.18760s. GPU RAM: 0.56/10.76 GB
[2023-06-19 11:15:36] INFO Training: Epoch=  5 [train_loss_0=0.2563 is_clicked logloss=0.6488 accuracy=0.6359 f1=0.0000 precision=0.0000 recall=0.0000 auc=0.6249 train_loss_0=0.4577 is_installed logloss=0.3826 accuracy=0.8186 f1=0.3335 precision=0.5431 recall=0.2410 auc=0.8147 train_loss_0=0.2563]
[2023-06-19 11:15:36] INFO Train time: 71.37382s. Valid time: 0.19082s. GPU RAM: 0.56/10.76 GB
[2023-06-19 11:16:47] INFO Training: Epoch=  6 [train_loss_0=0.2539 is_clicked logloss=0.6495 accuracy=0.6359 f1=0.0000 precision=0.0000 recall=0.0000 auc=0.6473 train_loss_0=0.4599 is_installed logloss=0.3898 accuracy=0.8212 f1=0.2818 precision=0.5821 recall=0.1861 auc=0.8095 train_loss_0=0.2539]
[2023-06-19 11:16:47] INFO Train time: 70.90399s. Valid time: 0.18208s. GPU RAM: 0.56/10.76 GB
[2023-06-19 11:17:58] INFO Training: Epoch=  7 [train_loss_0=0.2515 is_clicked logloss=0.6538 accuracy=0.6359 f1=0.0000 precision=0.0000 recall=0.0000 auc=0.6287 train_loss_0=0.4617 is_installed logloss=0.3956 accuracy=0.8217 f1=0.2054 precision=0.6429 recall=0.1224 auc=0.8150 train_loss_0=0.2515]
[2023-06-19 11:17:58] INFO Train time: 70.35770s. Valid time: 0.20388s. GPU RAM: 0.56/10.76 GB
[2023-06-19 11:19:10] INFO Training: Epoch=  8 [train_loss_0=0.2498 is_clicked logloss=0.6524 accuracy=0.6359 f1=0.0000 precision=0.0000 recall=0.0000 auc=0.6146 train_loss_0=0.4633 is_installed logloss=0.3826 accuracy=0.8239 f1=0.3229 precision=0.5869 recall=0.2231 auc=0.8208 train_loss_0=0.2498]
[2023-06-19 11:19:10] INFO Train time: 72.31879s. Valid time: 0.18049s. GPU RAM: 0.56/10.76 GB
[2023-06-19 11:20:21] INFO Training: Epoch=  9 [train_loss_0=0.2475 is_clicked logloss=0.6489 accuracy=0.6359 f1=0.0000 precision=0.0000 recall=0.0000 auc=0.6370 train_loss_0=0.4656 is_installed logloss=0.3868 accuracy=0.8221 f1=0.2896 precision=0.5867 recall=0.1924 auc=0.8135 train_loss_0=0.2475]
[2023-06-19 11:20:21] INFO Train time: 70.26085s. Valid time: 0.18983s. GPU RAM: 0.56/10.76 GB
[2023-06-19 11:21:32] INFO Training: Epoch= 10 [train_loss_0=0.2448 is_clicked logloss=0.6554 accuracy=0.6359 f1=0.0000 precision=0.0000 recall=0.0000 auc=0.6280 train_loss_0=0.4672 is_installed logloss=0.3881 accuracy=0.8220 f1=0.2619 precision=0.5999 recall=0.1679 auc=0.8144 train_loss_0=0.2448]
[2023-06-19 11:21:32] INFO Train time: 70.71829s. Valid time: 0.18618s. GPU RAM: 0.56/10.76 GB
[2023-06-19 11:21:32] INFO Early stopped. Since the metric logloss haven't been improved for 10 epochs.
[2023-06-19 11:21:32] INFO The best score of logloss is 0.3701 on epoch 0
[2023-06-19 11:21:32] INFO Best model checkpoint saved in ./saved/PLE/recsys/2023-06-19-11-08-31.ckpt.
