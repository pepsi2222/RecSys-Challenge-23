[2023-06-19 16:42:14] INFO Log saved in /data2/home/xingmei/RecSys23/log/PLE/finetune/2023-06-19-16-42-14.log.
[2023-06-19 16:42:14] INFO Global seed set to 2022
[2023-06-19 16:42:14] INFO dataset is read from /data2/home/xingmei/RecSys23/data.
[2023-06-19 16:43:33] INFO 
Dataset Info: 

===================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================
interaction information: 
field        weekday      f_1          f_2          f_3          f_4          f_5          f_6          f_8          f_9          f_10         f_11         f_12         f_13         f_14         f_15         f_16         f_17         f_18         f_19         f_20         f_21         f_22         f_23         f_24         f_25         f_26         f_30         f_31         f_32         f_33         f_34         f_35         f_36         f_37         f_38         f_39         f_40         f_41         f_42         f_43         f_44         f_45         f_46         f_47         f_48         f_49         f_50         f_51         f_52         f_53         f_54         f_55         f_56         f_57         f_58         f_59         f_60         f_61         f_62         f_63         f_64         f_65         f_66         f_67         f_68         f_69         f_70         f_71         f_72         f_73         f_74         f_75         f_76         f_77         f_78         f_79         is_clicked   is_installed 
type         token        float        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        float        token        token        token        token        token        token        token        token        token        token        token        token        token        token        float        float        token        token        token        token        float        float        float        float        float        float        float        token        token        token        token        token        token        token        token        token        float        float        
##           8            -            136          6            631          7            4455         7            8            4            25           25           206          19           4277         11           42           543          20           56           35           25           5            5            4            3            3            3            5            3            3            3            3            3            3            3            3            3            6725         -            18           19           11           21           21           15           22           1725         133          85           172          306          182          415          -            -            232          584          1032         298          -            -            -            -            -            -            -            5            12           9            5            32           9            5            11           7            -            -            
===================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================
Total Interactions: 1045756
===================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================
f_1=StandardScaler()
f_43=StandardScaler()
f_58=StandardScaler()
f_59=StandardScaler()
f_64=MinMaxScaler()
f_65=StandardScaler()
f_66=StandardScaler()
f_67=StandardScaler()
f_68=StandardScaler()
f_69=StandardScaler()
f_70=StandardScaler()
[2023-06-19 16:43:33] INFO 
Model Config: 

data:
	binarized_rating_thres=None
	fm_eval=False
	neg_count=0
	sampler=None
	shuffle=False
	split_mode=entry
	split_ratio=[1016364, 29392, 0]
	fmeval=True
	low_rating_thres=None
eval:
	batch_size=4096
	cutoff=[5, 10, 20]
	val_metrics=['logloss', 'auc', 'accuracy', 'f1', 'precision', 'recall']
	val_n_epoch=1
	test_metrics=['logloss', 'auc', 'accuracy', 'f1', 'precision', 'recall']
	topk=100
	save_path=./saved/
	binarized_prob_thres=0.5
	main_task=is_installed
model:
	embed_dim=64
	item_bias=False
	num_levels=2
	specific_experts_per_task=4
	num_shared_experts=3
	expert_mlp_layer=[128, 128]
	expert_activation=relu
	expert_dropout=0.1
	gate_mlp_layer=[128]
	gate_activation=relu
	gate_dropout=0.1
	tower_mlp_layer=[128]
	tower_activation=relu
	tower_dropout=0.1
	tower_batch_norm=False
train:
	accelerator=gpu
	ann=None
	batch_size=512
	early_stop_mode=min
	early_stop_patience=10
	epochs=1000
	gpu=[3]
	grad_clip_norm=None
	init_method=xavier_normal
	item_batch_size=1024
	learner=adam
	learning_rate=0.001
	num_threads=10
	sampling_method=none
	sampler=uniform
	negative_count=0
	excluding_hist=False
	scheduler=None
	seed=2022
	weight_decay=1e-05
	tensorboard_path=None
	weights=[1.0, 20.0]
[2023-06-19 16:43:33] INFO save_dir:./saved/
[2023-06-19 16:43:33] INFO PLE(
  (loss_fn): BCEWithLogitLoss()
  (embedding): Embeddings(
    num_features=76, embed_dim=64, reduction=mean
    (embeddings): ModuleDict(
      (f_79): Embedding(7, 64, padding_idx=0)
      (f_78): Embedding(11, 64, padding_idx=0)
      (f_62): Embedding(1032, 64, padding_idx=0)
      (f_76): Embedding(9, 64, padding_idx=0)
      (f_40): Embedding(3, 64, padding_idx=0)
      (f_14): Embedding(19, 64, padding_idx=0)
      (f_32): Embedding(5, 64, padding_idx=0)
      (f_30): Embedding(3, 64, padding_idx=0)
      (f_59): DenseEmbedding(
        embedding_dim=64, bias=False, batch_norm=True
        (batch_norm_layer): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (weight): Linear(in_features=1, out_features=64, bias=False)
      )
      (f_72): Embedding(12, 64, padding_idx=0)
      (f_24): Embedding(5, 64, padding_idx=0)
      (f_26): Embedding(3, 64, padding_idx=0)
      (f_16): Embedding(11, 64, padding_idx=0)
      (f_70): DenseEmbedding(
        embedding_dim=64, bias=False, batch_norm=True
        (batch_norm_layer): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (weight): Linear(in_features=1, out_features=64, bias=False)
      )
      (f_21): Embedding(35, 64, padding_idx=0)
      (f_69): DenseEmbedding(
        embedding_dim=64, bias=False, batch_norm=True
        (batch_norm_layer): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (weight): Linear(in_features=1, out_features=64, bias=False)
      )
      (f_39): Embedding(3, 64, padding_idx=0)
      (f_3): Embedding(6, 64, padding_idx=0)
      (f_49): Embedding(15, 64, padding_idx=0)
      (f_37): Embedding(3, 64, padding_idx=0)
      (f_33): Embedding(3, 64, padding_idx=0)
      (f_19): Embedding(20, 64, padding_idx=0)
      (f_61): Embedding(584, 64, padding_idx=0)
      (f_64): DenseEmbedding(
        embedding_dim=64, bias=False, batch_norm=True
        (batch_norm_layer): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (weight): Linear(in_features=1, out_features=64, bias=False)
      )
      (f_2): Embedding(136, 64, padding_idx=0)
      (f_60): Embedding(232, 64, padding_idx=0)
      (f_18): Embedding(543, 64, padding_idx=0)
      (f_71): Embedding(5, 64, padding_idx=0)
      (f_35): Embedding(3, 64, padding_idx=0)
      (f_57): Embedding(415, 64, padding_idx=0)
      (f_48): Embedding(21, 64, padding_idx=0)
      (f_73): Embedding(9, 64, padding_idx=0)
      (weekday): Embedding(8, 64, padding_idx=0)
      (f_46): Embedding(11, 64, padding_idx=0)
      (f_25): Embedding(4, 64, padding_idx=0)
      (f_65): DenseEmbedding(
        embedding_dim=64, bias=False, batch_norm=True
        (batch_norm_layer): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (weight): Linear(in_features=1, out_features=64, bias=False)
      )
      (f_41): Embedding(3, 64, padding_idx=0)
      (f_4): Embedding(631, 64, padding_idx=0)
      (f_20): Embedding(56, 64, padding_idx=0)
      (f_42): Embedding(6725, 64, padding_idx=0)
      (f_74): Embedding(5, 64, padding_idx=0)
      (f_66): DenseEmbedding(
        embedding_dim=64, bias=False, batch_norm=True
        (batch_norm_layer): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (weight): Linear(in_features=1, out_features=64, bias=False)
      )
      (f_23): Embedding(5, 64, padding_idx=0)
      (f_13): Embedding(206, 64, padding_idx=0)
      (f_31): Embedding(3, 64, padding_idx=0)
      (f_6): Embedding(4455, 64, padding_idx=0)
      (f_1): DenseEmbedding(
        embedding_dim=64, bias=False, batch_norm=True
        (batch_norm_layer): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (weight): Linear(in_features=1, out_features=64, bias=False)
      )
      (f_47): Embedding(21, 64, padding_idx=0)
      (f_54): Embedding(172, 64, padding_idx=0)
      (f_11): Embedding(25, 64, padding_idx=0)
      (f_36): Embedding(3, 64, padding_idx=0)
      (f_15): Embedding(4277, 64, padding_idx=0)
      (f_50): Embedding(22, 64, padding_idx=0)
      (f_34): Embedding(3, 64, padding_idx=0)
      (f_68): DenseEmbedding(
        embedding_dim=64, bias=False, batch_norm=True
        (batch_norm_layer): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (weight): Linear(in_features=1, out_features=64, bias=False)
      )
      (f_58): DenseEmbedding(
        embedding_dim=64, bias=False, batch_norm=True
        (batch_norm_layer): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (weight): Linear(in_features=1, out_features=64, bias=False)
      )
      (f_53): Embedding(85, 64, padding_idx=0)
      (f_8): Embedding(7, 64, padding_idx=0)
      (f_38): Embedding(3, 64, padding_idx=0)
      (f_52): Embedding(133, 64, padding_idx=0)
      (f_44): Embedding(18, 64, padding_idx=0)
      (f_63): Embedding(298, 64, padding_idx=0)
      (f_17): Embedding(42, 64, padding_idx=0)
      (f_75): Embedding(32, 64, padding_idx=0)
      (f_22): Embedding(25, 64, padding_idx=0)
      (f_55): Embedding(306, 64, padding_idx=0)
      (f_9): Embedding(8, 64, padding_idx=0)
      (f_77): Embedding(5, 64, padding_idx=0)
      (f_45): Embedding(19, 64, padding_idx=0)
      (f_56): Embedding(182, 64, padding_idx=0)
      (f_5): Embedding(7, 64, padding_idx=0)
      (f_12): Embedding(25, 64, padding_idx=0)
      (f_10): Embedding(4, 64, padding_idx=0)
      (f_67): DenseEmbedding(
        embedding_dim=64, bias=False, batch_norm=True
        (batch_norm_layer): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (weight): Linear(in_features=1, out_features=64, bias=False)
      )
      (f_51): Embedding(1725, 64, padding_idx=0)
      (f_43): DenseEmbedding(
        embedding_dim=64, bias=False, batch_norm=True
        (batch_norm_layer): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (weight): Linear(in_features=1, out_features=64, bias=False)
      )
    )
  )
  (extraction_layers): Sequential(
    (0): ExtractionLayer(
      specific_experts_per_task=4, num_task=2, num_shared_experts=3
      (specific_experts): ModuleList(
        (0-1): 2 x ModuleList(
          (0-3): 4 x MLPModule(
            (model): Sequential(
              (0): Dropout(p=0.1, inplace=False)
              (1): Linear(in_features=4864, out_features=128, bias=True)
              (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (3): ReLU()
              (4): Dropout(p=0.1, inplace=False)
              (5): Linear(in_features=128, out_features=128, bias=True)
              (6): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (7): ReLU()
            )
          )
        )
      )
      (shared_experts): ModuleList(
        (0-2): 3 x MLPModule(
          (model): Sequential(
            (0): Dropout(p=0.1, inplace=False)
            (1): Linear(in_features=4864, out_features=128, bias=True)
            (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (3): ReLU()
            (4): Dropout(p=0.1, inplace=False)
            (5): Linear(in_features=128, out_features=128, bias=True)
            (6): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (7): ReLU()
          )
        )
      )
      (gates): ModuleList(
        (0-1): 2 x MLPModule(
          (model): Sequential(
            (0): Dropout(p=0.1, inplace=False)
            (1): Linear(in_features=4864, out_features=128, bias=True)
            (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (3): ReLU()
            (4): Dropout(p=0.1, inplace=False)
            (5): Linear(in_features=128, out_features=7, bias=True)
            (6): BatchNorm1d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (7): ReLU()
            (8): Softmax(dim=-1)
          )
        )
      )
      (shared_gates): MLPModule(
        (model): Sequential(
          (0): Dropout(p=0.1, inplace=False)
          (1): Linear(in_features=4864, out_features=128, bias=True)
          (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (3): ReLU()
          (4): Dropout(p=0.1, inplace=False)
          (5): Linear(in_features=128, out_features=11, bias=True)
          (6): BatchNorm1d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (7): ReLU()
          (8): Softmax(dim=-1)
        )
      )
    )
    (1): ExtractionLayer(
      specific_experts_per_task=4, num_task=2, num_shared_experts=3
      (specific_experts): ModuleList(
        (0-1): 2 x ModuleList(
          (0-3): 4 x MLPModule(
            (model): Sequential(
              (0): Dropout(p=0.1, inplace=False)
              (1): Linear(in_features=128, out_features=128, bias=True)
              (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (3): ReLU()
              (4): Dropout(p=0.1, inplace=False)
              (5): Linear(in_features=128, out_features=128, bias=True)
              (6): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (7): ReLU()
            )
          )
        )
      )
      (shared_experts): ModuleList(
        (0-2): 3 x MLPModule(
          (model): Sequential(
            (0): Dropout(p=0.1, inplace=False)
            (1): Linear(in_features=128, out_features=128, bias=True)
            (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (3): ReLU()
            (4): Dropout(p=0.1, inplace=False)
            (5): Linear(in_features=128, out_features=128, bias=True)
            (6): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (7): ReLU()
          )
        )
      )
      (gates): ModuleList(
        (0-1): 2 x MLPModule(
          (model): Sequential(
            (0): Dropout(p=0.1, inplace=False)
            (1): Linear(in_features=128, out_features=128, bias=True)
            (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (3): ReLU()
            (4): Dropout(p=0.1, inplace=False)
            (5): Linear(in_features=128, out_features=7, bias=True)
            (6): BatchNorm1d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (7): ReLU()
            (8): Softmax(dim=-1)
          )
        )
      )
    )
  )
  (towers): ModuleDict(
    (is_clicked): MLPModule(
      (model): Sequential(
        (0): Dropout(p=0.1, inplace=False)
        (1): Linear(in_features=128, out_features=128, bias=True)
        (2): ReLU()
        (3): Dropout(p=0.1, inplace=False)
        (4): Linear(in_features=128, out_features=1, bias=True)
      )
    )
    (is_installed): MLPModule(
      (model): Sequential(
        (0): Dropout(p=0.1, inplace=False)
        (1): Linear(in_features=128, out_features=128, bias=True)
        (2): ReLU()
        (3): Dropout(p=0.1, inplace=False)
        (4): Linear(in_features=128, out_features=1, bias=True)
      )
    )
  )
)
[2023-06-19 16:43:33] INFO GPU id [3] are selected.
[2023-06-19 16:45:05] INFO Training: Epoch=  0 [train_loss_0=0.2793 is_clicked logloss=0.6931 accuracy=0.6359 f1=0.0000 precision=0.0000 recall=0.0000 auc=0.5000 train_loss_0=0.6951 is_installed logloss=0.3920 accuracy=0.8241 f1=0.2528 precision=0.6343 recall=0.1580 auc=0.8157 train_loss_0=0.2793]
[2023-06-19 16:45:05] INFO Train time: 90.47864s. Valid time: 0.21344s. GPU RAM: 0.55/10.76 GB
[2023-06-19 16:45:05] INFO logloss improved. Best value: 0.3920
[2023-06-19 16:46:36] INFO Training: Epoch=  1 [train_loss_0=0.2651 is_clicked logloss=0.6931 accuracy=0.6359 f1=0.0000 precision=0.0000 recall=0.0000 auc=0.5000 train_loss_0=0.6931 is_installed logloss=0.3905 accuracy=0.8192 f1=0.3519 precision=0.5435 recall=0.2607 auc=0.8070 train_loss_0=0.2651]
[2023-06-19 16:46:36] INFO Train time: 90.74022s. Valid time: 0.28050s. GPU RAM: 0.56/10.76 GB
[2023-06-19 16:46:36] INFO logloss improved. Best value: 0.3905
[2023-06-19 16:48:08] INFO Training: Epoch=  2 [train_loss_0=0.2615 is_clicked logloss=0.6931 accuracy=0.6359 f1=0.0000 precision=0.0000 recall=0.0000 auc=0.5000 train_loss_0=0.6931 is_installed logloss=0.3990 accuracy=0.8228 f1=0.2348 precision=0.6331 recall=0.1443 auc=0.8122 train_loss_0=0.2615]
[2023-06-19 16:48:08] INFO Train time: 91.67211s. Valid time: 0.27948s. GPU RAM: 0.56/10.76 GB
[2023-06-19 16:49:44] INFO Training: Epoch=  3 [train_loss_0=0.2586 is_clicked logloss=0.6931 accuracy=0.6359 f1=0.0000 precision=0.0000 recall=0.0000 auc=0.5000 train_loss_0=0.6931 is_installed logloss=0.4073 accuracy=0.8205 f1=0.2690 precision=0.5795 recall=0.1753 auc=0.7839 train_loss_0=0.2586]
[2023-06-19 16:49:44] INFO Train time: 95.35131s. Valid time: 0.21223s. GPU RAM: 0.56/10.76 GB
[2023-06-19 16:51:16] INFO Training: Epoch=  4 [train_loss_0=0.2561 is_clicked logloss=0.6931 accuracy=0.6359 f1=0.0000 precision=0.0000 recall=0.0000 auc=0.5000 train_loss_0=0.6931 is_installed logloss=0.4172 accuracy=0.8175 f1=0.2324 precision=0.5623 recall=0.1467 auc=0.7893 train_loss_0=0.2561]
[2023-06-19 16:51:16] INFO Train time: 92.07741s. Valid time: 0.20981s. GPU RAM: 0.56/10.76 GB
[2023-06-19 16:52:49] INFO Training: Epoch=  5 [train_loss_0=0.2536 is_clicked logloss=0.6931 accuracy=0.6359 f1=0.0000 precision=0.0000 recall=0.0000 auc=0.5000 train_loss_0=0.6931 is_installed logloss=0.4180 accuracy=0.8176 f1=0.2168 precision=0.5708 recall=0.1340 auc=0.8007 train_loss_0=0.2536]
[2023-06-19 16:52:49] INFO Train time: 92.66489s. Valid time: 0.21358s. GPU RAM: 0.56/10.76 GB
[2023-06-19 16:54:23] INFO Training: Epoch=  6 [train_loss_0=0.2514 is_clicked logloss=0.6931 accuracy=0.6359 f1=0.0000 precision=0.0000 recall=0.0000 auc=0.5000 train_loss_0=0.6931 is_installed logloss=0.4086 accuracy=0.8189 f1=0.2857 precision=0.5575 recall=0.1922 auc=0.7948 train_loss_0=0.2514]
[2023-06-19 16:54:23] INFO Train time: 93.75749s. Valid time: 0.23447s. GPU RAM: 0.56/10.76 GB
[2023-06-19 16:55:56] INFO Training: Epoch=  7 [train_loss_0=0.2492 is_clicked logloss=0.6931 accuracy=0.6359 f1=0.0000 precision=0.0000 recall=0.0000 auc=0.5000 train_loss_0=0.6931 is_installed logloss=0.4040 accuracy=0.8201 f1=0.2656 precision=0.5766 recall=0.1727 auc=0.8061 train_loss_0=0.2492]
[2023-06-19 16:55:56] INFO Train time: 92.49957s. Valid time: 0.20493s. GPU RAM: 0.56/10.76 GB
[2023-06-19 16:57:29] INFO Training: Epoch=  8 [train_loss_0=0.2466 is_clicked logloss=0.6931 accuracy=0.6359 f1=0.0000 precision=0.0000 recall=0.0000 auc=0.5000 train_loss_0=0.6931 is_installed logloss=0.4092 accuracy=0.8185 f1=0.2556 precision=0.5642 recall=0.1654 auc=0.7910 train_loss_0=0.2466]
[2023-06-19 16:57:29] INFO Train time: 92.27713s. Valid time: 0.21848s. GPU RAM: 0.56/10.76 GB
[2023-06-19 16:59:06] INFO Training: Epoch=  9 [train_loss_0=0.2442 is_clicked logloss=0.6931 accuracy=0.6359 f1=0.0000 precision=0.0000 recall=0.0000 auc=0.5000 train_loss_0=0.6931 is_installed logloss=0.4270 accuracy=0.8165 f1=0.2438 precision=0.5461 recall=0.1570 auc=0.7788 train_loss_0=0.2442]
[2023-06-19 16:59:06] INFO Train time: 96.82870s. Valid time: 0.22609s. GPU RAM: 0.56/10.76 GB
[2023-06-19 17:00:43] INFO Training: Epoch= 10 [train_loss_0=0.2413 is_clicked logloss=0.6931 accuracy=0.6359 f1=0.0000 precision=0.0000 recall=0.0000 auc=0.5000 train_loss_0=0.6931 is_installed logloss=0.4093 accuracy=0.8177 f1=0.2629 precision=0.5530 recall=0.1725 auc=0.8013 train_loss_0=0.2413]
[2023-06-19 17:00:43] INFO Train time: 97.20797s. Valid time: 0.23255s. GPU RAM: 0.56/10.76 GB
[2023-06-19 17:02:18] INFO Training: Epoch= 11 [train_loss_0=0.2382 is_clicked logloss=0.6931 accuracy=0.6359 f1=0.0000 precision=0.0000 recall=0.0000 auc=0.5000 train_loss_0=0.6931 is_installed logloss=0.4163 accuracy=0.8134 f1=0.2492 precision=0.5171 recall=0.1643 auc=0.7879 train_loss_0=0.2382]
[2023-06-19 17:02:18] INFO Train time: 94.81486s. Valid time: 0.20629s. GPU RAM: 0.56/10.76 GB
[2023-06-19 17:02:18] INFO Early stopped. Since the metric logloss haven't been improved for 10 epochs.
[2023-06-19 17:02:18] INFO The best score of logloss is 0.3905 on epoch 1
[2023-06-19 17:02:18] INFO Best model checkpoint saved in ./saved/PLE/recsys/2023-06-19-16-42-14.ckpt.
