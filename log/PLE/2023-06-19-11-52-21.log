[2023-06-19 11:52:21] INFO Log saved in /data2/home/xingmei/RecSys23/log/PLE/recsys/2023-06-19-11-52-21.log.
[2023-06-19 11:52:21] INFO Global seed set to 2022
[2023-06-19 11:52:21] INFO Load dataset from cache.
[2023-06-19 11:52:46] INFO 
Dataset Info: 

======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================
interaction information: 
field        f_1          f_2          f_3          f_4          f_5          f_6          f_8          f_9          f_10         f_11         f_12         f_13         f_14         f_15         f_16         f_17         f_18         f_19         f_20         f_21         f_22         f_23         f_24         f_25         f_26         f_30         f_31         f_32         f_33         f_34         f_35         f_36         f_37         f_38         f_39         f_40         f_41         f_42         f_43         f_44         f_45         f_46         f_47         f_48         f_49         f_50         f_51         f_52         f_53         f_54         f_55         f_56         f_57         f_58         f_59         f_60         f_61         f_62         f_63         f_64         f_65         f_66         f_67         f_68         f_69         f_70         f_71         f_72         f_73         f_74         f_75         f_76         f_77         f_78         f_79         is_clicked   is_installed 
type         float        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        float        token        token        token        token        token        token        token        token        token        token        token        token        token        token        float        float        token        token        token        token        float        float        float        float        float        float        float        token        token        token        token        token        token        token        token        token        float        float        
##           -            140          6            639          7            5235         7            8            4            25           27           332          20           5855         13           50           925          20           58           36           27           5            5            4            3            3            3            5            3            3            3            3            3            3            3            3            3            8883         -            22           24           12           28           28           21           35           1829         172          103          213          390          221          517          -            -            403          826          1397         408          -            -            -            -            -            -            -            5            12           9            5            32           9            5            14           8            -            -            
======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================
Total Interactions: 3646825
======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================
f_1=StandardScaler()
f_43=StandardScaler()
f_58=StandardScaler()
f_59=StandardScaler()
f_64=MinMaxScaler()
f_65=StandardScaler()
f_66=StandardScaler()
f_67=StandardScaler()
f_68=StandardScaler()
f_69=StandardScaler()
f_70=StandardScaler()
[2023-06-19 11:52:46] INFO 
Model Config: 

data:
	binarized_rating_thres=None
	fm_eval=False
	neg_count=0
	sampler=None
	shuffle=False
	split_mode=entry
	split_ratio=[3387880, 97972, 160973]
	fmeval=True
	low_rating_thres=None
eval:
	batch_size=4096
	cutoff=[5, 10, 20]
	val_metrics=['logloss', 'auc', 'accuracy', 'f1', 'precision', 'recall']
	val_n_epoch=1
	test_metrics=['logloss', 'auc', 'accuracy', 'f1', 'precision', 'recall']
	topk=100
	save_path=./saved/
	binarized_prob_thres=0.5
	main_task=is_installed
model:
	embed_dim=64
	item_bias=False
	num_levels=2
	specific_experts_per_task=4
	num_shared_experts=3
	expert_mlp_layer=[128, 128]
	expert_activation=relu
	expert_dropout=0.1
	gate_mlp_layer=[128]
	gate_activation=relu
	gate_dropout=0.1
	tower_mlp_layer=[128]
	tower_activation=relu
	tower_dropout=0.1
	tower_batch_norm=False
train:
	accelerator=gpu
	ann=None
	batch_size=512
	early_stop_mode=min
	early_stop_patience=10
	epochs=1000
	gpu=[0]
	grad_clip_norm=None
	init_method=xavier_normal
	item_batch_size=1024
	learner=adam
	learning_rate=0.001
	num_threads=10
	sampling_method=none
	sampler=uniform
	negative_count=0
	excluding_hist=False
	scheduler=None
	seed=2022
	weight_decay=1e-05
	tensorboard_path=None
	weights=[1.0, 20.0]
[2023-06-19 11:52:46] INFO save_dir:./saved/
[2023-06-19 11:52:46] INFO PLE(
  (loss_fn): BCEWithLogitLoss()
  (embedding): Embeddings(
    num_features=75, embed_dim=64, reduction=mean
    (embeddings): ModuleDict(
      (f_48): Embedding(28, 64, padding_idx=0)
      (f_44): Embedding(22, 64, padding_idx=0)
      (f_79): Embedding(8, 64, padding_idx=0)
      (f_49): Embedding(21, 64, padding_idx=0)
      (f_34): Embedding(3, 64, padding_idx=0)
      (f_36): Embedding(3, 64, padding_idx=0)
      (f_74): Embedding(5, 64, padding_idx=0)
      (f_26): Embedding(3, 64, padding_idx=0)
      (f_55): Embedding(390, 64, padding_idx=0)
      (f_30): Embedding(3, 64, padding_idx=0)
      (f_60): Embedding(403, 64, padding_idx=0)
      (f_62): Embedding(1397, 64, padding_idx=0)
      (f_73): Embedding(9, 64, padding_idx=0)
      (f_18): Embedding(925, 64, padding_idx=0)
      (f_61): Embedding(826, 64, padding_idx=0)
      (f_58): DenseEmbedding(
        embedding_dim=64, bias=False, batch_norm=True
        (batch_norm_layer): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (weight): Linear(in_features=1, out_features=64, bias=False)
      )
      (f_14): Embedding(20, 64, padding_idx=0)
      (f_15): Embedding(5855, 64, padding_idx=0)
      (f_4): Embedding(639, 64, padding_idx=0)
      (f_43): DenseEmbedding(
        embedding_dim=64, bias=False, batch_norm=True
        (batch_norm_layer): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (weight): Linear(in_features=1, out_features=64, bias=False)
      )
      (f_70): DenseEmbedding(
        embedding_dim=64, bias=False, batch_norm=True
        (batch_norm_layer): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (weight): Linear(in_features=1, out_features=64, bias=False)
      )
      (f_3): Embedding(6, 64, padding_idx=0)
      (f_53): Embedding(103, 64, padding_idx=0)
      (f_13): Embedding(332, 64, padding_idx=0)
      (f_12): Embedding(27, 64, padding_idx=0)
      (f_38): Embedding(3, 64, padding_idx=0)
      (f_39): Embedding(3, 64, padding_idx=0)
      (f_56): Embedding(221, 64, padding_idx=0)
      (f_40): Embedding(3, 64, padding_idx=0)
      (f_46): Embedding(12, 64, padding_idx=0)
      (f_17): Embedding(50, 64, padding_idx=0)
      (f_57): Embedding(517, 64, padding_idx=0)
      (f_16): Embedding(13, 64, padding_idx=0)
      (f_76): Embedding(9, 64, padding_idx=0)
      (f_52): Embedding(172, 64, padding_idx=0)
      (f_54): Embedding(213, 64, padding_idx=0)
      (f_31): Embedding(3, 64, padding_idx=0)
      (f_5): Embedding(7, 64, padding_idx=0)
      (f_66): DenseEmbedding(
        embedding_dim=64, bias=False, batch_norm=True
        (batch_norm_layer): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (weight): Linear(in_features=1, out_features=64, bias=False)
      )
      (f_6): Embedding(5235, 64, padding_idx=0)
      (f_19): Embedding(20, 64, padding_idx=0)
      (f_72): Embedding(12, 64, padding_idx=0)
      (f_37): Embedding(3, 64, padding_idx=0)
      (f_33): Embedding(3, 64, padding_idx=0)
      (f_1): DenseEmbedding(
        embedding_dim=64, bias=False, batch_norm=True
        (batch_norm_layer): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (weight): Linear(in_features=1, out_features=64, bias=False)
      )
      (f_47): Embedding(28, 64, padding_idx=0)
      (f_25): Embedding(4, 64, padding_idx=0)
      (f_41): Embedding(3, 64, padding_idx=0)
      (f_75): Embedding(32, 64, padding_idx=0)
      (f_10): Embedding(4, 64, padding_idx=0)
      (f_50): Embedding(35, 64, padding_idx=0)
      (f_42): Embedding(8883, 64, padding_idx=0)
      (f_78): Embedding(14, 64, padding_idx=0)
      (f_51): Embedding(1829, 64, padding_idx=0)
      (f_59): DenseEmbedding(
        embedding_dim=64, bias=False, batch_norm=True
        (batch_norm_layer): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (weight): Linear(in_features=1, out_features=64, bias=False)
      )
      (f_23): Embedding(5, 64, padding_idx=0)
      (f_35): Embedding(3, 64, padding_idx=0)
      (f_64): DenseEmbedding(
        embedding_dim=64, bias=False, batch_norm=True
        (batch_norm_layer): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (weight): Linear(in_features=1, out_features=64, bias=False)
      )
      (f_21): Embedding(36, 64, padding_idx=0)
      (f_67): DenseEmbedding(
        embedding_dim=64, bias=False, batch_norm=True
        (batch_norm_layer): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (weight): Linear(in_features=1, out_features=64, bias=False)
      )
      (f_9): Embedding(8, 64, padding_idx=0)
      (f_45): Embedding(24, 64, padding_idx=0)
      (f_24): Embedding(5, 64, padding_idx=0)
      (f_68): DenseEmbedding(
        embedding_dim=64, bias=False, batch_norm=True
        (batch_norm_layer): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (weight): Linear(in_features=1, out_features=64, bias=False)
      )
      (f_71): Embedding(5, 64, padding_idx=0)
      (f_65): DenseEmbedding(
        embedding_dim=64, bias=False, batch_norm=True
        (batch_norm_layer): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (weight): Linear(in_features=1, out_features=64, bias=False)
      )
      (f_11): Embedding(25, 64, padding_idx=0)
      (f_2): Embedding(140, 64, padding_idx=0)
      (f_77): Embedding(5, 64, padding_idx=0)
      (f_69): DenseEmbedding(
        embedding_dim=64, bias=False, batch_norm=True
        (batch_norm_layer): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (weight): Linear(in_features=1, out_features=64, bias=False)
      )
      (f_22): Embedding(27, 64, padding_idx=0)
      (f_8): Embedding(7, 64, padding_idx=0)
      (f_32): Embedding(5, 64, padding_idx=0)
      (f_63): Embedding(408, 64, padding_idx=0)
      (f_20): Embedding(58, 64, padding_idx=0)
    )
  )
  (extraction_layers): Sequential(
    (0): ExtractionLayer(
      specific_experts_per_task=4, num_task=2, num_shared_experts=3
      (specific_experts): ModuleList(
        (0-1): 2 x ModuleList(
          (0-3): 4 x MLPModule(
            (model): Sequential(
              (0): Dropout(p=0.1, inplace=False)
              (1): Linear(in_features=4800, out_features=128, bias=True)
              (2): ReLU()
              (3): Dropout(p=0.1, inplace=False)
              (4): Linear(in_features=128, out_features=128, bias=True)
              (5): ReLU()
            )
          )
        )
      )
      (shared_experts): ModuleList(
        (0-2): 3 x MLPModule(
          (model): Sequential(
            (0): Dropout(p=0.1, inplace=False)
            (1): Linear(in_features=4800, out_features=128, bias=True)
            (2): ReLU()
            (3): Dropout(p=0.1, inplace=False)
            (4): Linear(in_features=128, out_features=128, bias=True)
            (5): ReLU()
          )
        )
      )
      (gates): ModuleList(
        (0-1): 2 x MLPModule(
          (model): Sequential(
            (0): Dropout(p=0.1, inplace=False)
            (1): Linear(in_features=4800, out_features=128, bias=True)
            (2): ReLU()
            (3): Dropout(p=0.1, inplace=False)
            (4): Linear(in_features=128, out_features=7, bias=True)
            (5): ReLU()
            (6): Softmax(dim=-1)
          )
        )
      )
      (shared_gates): MLPModule(
        (model): Sequential(
          (0): Dropout(p=0.1, inplace=False)
          (1): Linear(in_features=4800, out_features=128, bias=True)
          (2): ReLU()
          (3): Dropout(p=0.1, inplace=False)
          (4): Linear(in_features=128, out_features=11, bias=True)
          (5): ReLU()
          (6): Softmax(dim=-1)
        )
      )
    )
    (1): ExtractionLayer(
      specific_experts_per_task=4, num_task=2, num_shared_experts=3
      (specific_experts): ModuleList(
        (0-1): 2 x ModuleList(
          (0-3): 4 x MLPModule(
            (model): Sequential(
              (0): Dropout(p=0.1, inplace=False)
              (1): Linear(in_features=128, out_features=128, bias=True)
              (2): ReLU()
              (3): Dropout(p=0.1, inplace=False)
              (4): Linear(in_features=128, out_features=128, bias=True)
              (5): ReLU()
            )
          )
        )
      )
      (shared_experts): ModuleList(
        (0-2): 3 x MLPModule(
          (model): Sequential(
            (0): Dropout(p=0.1, inplace=False)
            (1): Linear(in_features=128, out_features=128, bias=True)
            (2): ReLU()
            (3): Dropout(p=0.1, inplace=False)
            (4): Linear(in_features=128, out_features=128, bias=True)
            (5): ReLU()
          )
        )
      )
      (gates): ModuleList(
        (0-1): 2 x MLPModule(
          (model): Sequential(
            (0): Dropout(p=0.1, inplace=False)
            (1): Linear(in_features=128, out_features=128, bias=True)
            (2): ReLU()
            (3): Dropout(p=0.1, inplace=False)
            (4): Linear(in_features=128, out_features=7, bias=True)
            (5): ReLU()
            (6): Softmax(dim=-1)
          )
        )
      )
    )
  )
  (towers): ModuleDict(
    (is_clicked): MLPModule(
      (model): Sequential(
        (0): Dropout(p=0.1, inplace=False)
        (1): Linear(in_features=128, out_features=128, bias=True)
        (2): ReLU()
        (3): Dropout(p=0.1, inplace=False)
        (4): Linear(in_features=128, out_features=1, bias=True)
      )
    )
    (is_installed): MLPModule(
      (model): Sequential(
        (0): Dropout(p=0.1, inplace=False)
        (1): Linear(in_features=128, out_features=128, bias=True)
        (2): ReLU()
        (3): Dropout(p=0.1, inplace=False)
        (4): Linear(in_features=128, out_features=1, bias=True)
      )
    )
  )
)
[2023-06-19 11:52:46] INFO GPU id [0] are selected.
[2023-06-19 11:56:38] INFO Training: Epoch=  0 [train_loss_0=0.2721 is_clicked logloss=0.6931 accuracy=0.6375 f1=0.0000 precision=0.0000 recall=0.0000 auc=0.5000 train_loss_0=0.6929 is_installed logloss=0.3897 accuracy=0.8229 f1=0.3101 precision=0.5748 recall=0.2125 auc=0.8114 train_loss_0=0.2721]
[2023-06-19 11:56:38] INFO Train time: 230.70916s. Valid time: 0.55058s. GPU RAM: 0.56/10.76 GB
[2023-06-19 11:56:38] INFO logloss improved. Best value: 0.3897
[2023-06-19 12:00:27] INFO Training: Epoch=  1 [train_loss_0=0.2616 is_clicked logloss=0.6931 accuracy=0.6375 f1=0.0000 precision=0.0000 recall=0.0000 auc=0.5000 train_loss_0=0.6931 is_installed logloss=0.3964 accuracy=0.8232 f1=0.2165 precision=0.6383 recall=0.1304 auc=0.8142 train_loss_0=0.2616]
[2023-06-19 12:00:27] INFO Train time: 227.99129s. Valid time: 0.74509s. GPU RAM: 0.60/10.76 GB
[2023-06-19 12:04:18] INFO Training: Epoch=  2 [train_loss_0=0.2587 is_clicked logloss=0.6931 accuracy=0.6375 f1=0.0000 precision=0.0000 recall=0.0000 auc=0.5000 train_loss_0=0.6931 is_installed logloss=0.3801 accuracy=0.8247 f1=0.2253 precision=0.6552 recall=0.1361 auc=0.8259 train_loss_0=0.2587]
[2023-06-19 12:04:18] INFO Train time: 230.41196s. Valid time: 0.53113s. GPU RAM: 0.60/10.76 GB
[2023-06-19 12:04:20] INFO logloss improved. Best value: 0.3801
[2023-06-19 12:08:11] INFO Training: Epoch=  3 [train_loss_0=0.2573 is_clicked logloss=0.6931 accuracy=0.6375 f1=0.0000 precision=0.0000 recall=0.0000 auc=0.5000 train_loss_0=0.6931 is_installed logloss=0.3888 accuracy=0.8240 f1=0.2571 precision=0.6152 recall=0.1626 auc=0.8218 train_loss_0=0.2573]
[2023-06-19 12:08:11] INFO Train time: 230.94418s. Valid time: 0.60102s. GPU RAM: 0.62/10.76 GB
[2023-06-19 12:12:05] INFO Training: Epoch=  4 [train_loss_0=0.2564 is_clicked logloss=0.6931 accuracy=0.6375 f1=0.0000 precision=0.0000 recall=0.0000 auc=0.5000 train_loss_0=0.6931 is_installed logloss=0.3745 accuracy=0.8228 f1=0.2077 precision=0.6405 recall=0.1240 auc=0.8330 train_loss_0=0.2564]
[2023-06-19 12:12:05] INFO Train time: 232.78577s. Valid time: 0.57571s. GPU RAM: 0.62/10.76 GB
[2023-06-19 12:12:07] INFO logloss improved. Best value: 0.3745
[2023-06-19 12:15:59] INFO Training: Epoch=  5 [train_loss_0=0.2558 is_clicked logloss=0.6931 accuracy=0.6375 f1=0.0000 precision=0.0000 recall=0.0000 auc=0.5000 train_loss_0=0.6931 is_installed logloss=0.3883 accuracy=0.8225 f1=0.2616 precision=0.5935 recall=0.1679 auc=0.8110 train_loss_0=0.2558]
[2023-06-19 12:15:59] INFO Train time: 231.99361s. Valid time: 0.71156s. GPU RAM: 0.62/10.76 GB
[2023-06-19 12:19:54] INFO Training: Epoch=  6 [train_loss_0=0.2554 is_clicked logloss=0.6931 accuracy=0.6375 f1=0.0000 precision=0.0000 recall=0.0000 auc=0.5000 train_loss_0=0.6931 is_installed logloss=0.3801 accuracy=0.8258 f1=0.2703 precision=0.6286 recall=0.1723 auc=0.8289 train_loss_0=0.2554]
[2023-06-19 12:19:54] INFO Train time: 233.60980s. Valid time: 0.58591s. GPU RAM: 0.62/10.76 GB
[2023-06-19 12:23:47] INFO Training: Epoch=  7 [train_loss_0=0.2552 is_clicked logloss=0.6931 accuracy=0.6375 f1=0.0000 precision=0.0000 recall=0.0000 auc=0.5000 train_loss_0=0.6931 is_installed logloss=0.3826 accuracy=0.8219 f1=0.2941 precision=0.5726 recall=0.1980 auc=0.8203 train_loss_0=0.2552]
[2023-06-19 12:23:47] INFO Train time: 232.27574s. Valid time: 0.58430s. GPU RAM: 0.62/10.76 GB
[2023-06-19 12:27:40] INFO Training: Epoch=  8 [train_loss_0=0.2550 is_clicked logloss=0.6931 accuracy=0.6375 f1=0.0000 precision=0.0000 recall=0.0000 auc=0.5000 train_loss_0=0.6931 is_installed logloss=0.3809 accuracy=0.8212 f1=0.2845 precision=0.5690 recall=0.1897 auc=0.8149 train_loss_0=0.2550]
[2023-06-19 12:27:40] INFO Train time: 232.45962s. Valid time: 0.57554s. GPU RAM: 0.62/10.76 GB
[2023-06-19 12:31:32] INFO Training: Epoch=  9 [train_loss_0=0.2546 is_clicked logloss=0.6931 accuracy=0.6375 f1=0.0000 precision=0.0000 recall=0.0000 auc=0.5000 train_loss_0=0.6931 is_installed logloss=0.3766 accuracy=0.8248 f1=0.2463 precision=0.6360 recall=0.1528 auc=0.8247 train_loss_0=0.2546]
[2023-06-19 12:31:32] INFO Train time: 231.04126s. Valid time: 0.60100s. GPU RAM: 0.62/10.76 GB
[2023-06-19 12:35:24] INFO Training: Epoch= 10 [train_loss_0=0.2545 is_clicked logloss=0.6931 accuracy=0.6375 f1=0.0000 precision=0.0000 recall=0.0000 auc=0.5000 train_loss_0=0.6931 is_installed logloss=0.3792 accuracy=0.8226 f1=0.2661 precision=0.5930 recall=0.1716 auc=0.8254 train_loss_0=0.2545]
[2023-06-19 12:35:24] INFO Train time: 231.40350s. Valid time: 0.57605s. GPU RAM: 0.62/10.76 GB
[2023-06-19 12:39:20] INFO Training: Epoch= 11 [train_loss_0=0.2544 is_clicked logloss=0.6931 accuracy=0.6375 f1=0.0000 precision=0.0000 recall=0.0000 auc=0.5000 train_loss_0=0.6931 is_installed logloss=0.3769 accuracy=0.8228 f1=0.2730 precision=0.5905 recall=0.1777 auc=0.8212 train_loss_0=0.2544]
[2023-06-19 12:39:20] INFO Train time: 235.37884s. Valid time: 0.57515s. GPU RAM: 0.62/10.76 GB
[2023-06-19 12:43:14] INFO Training: Epoch= 12 [train_loss_0=0.2543 is_clicked logloss=0.6931 accuracy=0.6375 f1=0.0000 precision=0.0000 recall=0.0000 auc=0.5000 train_loss_0=0.6931 is_installed logloss=0.3813 accuracy=0.8210 f1=0.2464 precision=0.5845 recall=0.1562 auc=0.8214 train_loss_0=0.2543]
[2023-06-19 12:43:14] INFO Train time: 233.09192s. Valid time: 0.58150s. GPU RAM: 0.62/10.76 GB
[2023-06-19 12:47:07] INFO Training: Epoch= 13 [train_loss_0=0.2541 is_clicked logloss=0.6931 accuracy=0.6375 f1=0.0000 precision=0.0000 recall=0.0000 auc=0.5000 train_loss_0=0.6931 is_installed logloss=0.3806 accuracy=0.8244 f1=0.2724 precision=0.6100 recall=0.1755 auc=0.8215 train_loss_0=0.2541]
[2023-06-19 12:47:07] INFO Train time: 232.39414s. Valid time: 0.59694s. GPU RAM: 0.62/10.76 GB
[2023-06-19 12:51:01] INFO Training: Epoch= 14 [train_loss_0=0.2540 is_clicked logloss=0.6931 accuracy=0.6375 f1=0.0000 precision=0.0000 recall=0.0000 auc=0.5000 train_loss_0=0.6931 is_installed logloss=0.3846 accuracy=0.8241 f1=0.2369 precision=0.6337 recall=0.1457 auc=0.8228 train_loss_0=0.2540]
[2023-06-19 12:51:01] INFO Train time: 233.42032s. Valid time: 0.58452s. GPU RAM: 0.62/10.76 GB
[2023-06-19 12:51:01] INFO Early stopped. Since the metric logloss haven't been improved for 10 epochs.
[2023-06-19 12:51:01] INFO The best score of logloss is 0.3745 on epoch 4
[2023-06-19 12:51:01] INFO Best model checkpoint saved in ./saved/PLE/recsys/2023-06-19-11-52-21.ckpt.
[2023-06-19 12:51:23] INFO Predictions saved in ./predictions/PLE/2023-06-19-12-51-01['is_clicked', 'is_installed'].csv
