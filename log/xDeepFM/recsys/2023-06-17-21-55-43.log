[2023-06-17 21:55:43] INFO Log saved in /data2/home/xingmei/RecSys23/log/xDeepFM/recsys/2023-06-17-21-55-43.log.
[2023-06-17 21:55:43] INFO Global seed set to 2022
[2023-06-17 21:55:43] INFO Load dataset from cache.
[2023-06-17 21:55:45] INFO 
Dataset Info: 

=========================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================
interaction information: 
field        f_1          f_2          f_3          f_4          f_5          f_6          f_8          f_9          f_10         f_11         f_12         f_13         f_14         f_15         f_16         f_17         f_18         f_19         f_20         f_21         f_22         f_23         f_24         f_25         f_26         f_30         f_31         f_32         f_33         f_34         f_35         f_36         f_37         f_38         f_39         f_40         f_41         f_42         f_43         f_44         f_45         f_46         f_47         f_48         f_49         f_50         f_51         f_52         f_53         f_54         f_55         f_56         f_57         f_58         f_59         f_60         f_61         f_62         f_63         f_64         f_65         f_66         f_67         f_68         f_69         f_70         f_71         f_72         f_73         f_74         f_75         f_76         f_77         f_78         f_79         is_installed 
type         float        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        token        float        token        token        token        token        token        token        token        token        token        token        token        token        token        token        float        float        token        token        token        token        float        float        float        float        float        float        float        token        token        token        token        token        token        token        token        token        float        
##           -            140          6            639          7            5235         7            8            4            25           27           332          20           5855         13           50           925          20           58           36           27           5            5            4            3            3            3            5            3            3            3            3            3            3            3            3            3            8883         -            22           24           12           28           28           21           35           1829         172          103          213          390          221          517          -            -            403          826          1397         408          -            -            -            -            -            -            -            5            12           9            5            32           9            5            14           8            -            
=========================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================
Total Interactions: 3646825
=========================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================
f_1=StandardScaler()
f_43=StandardScaler()
f_58=StandardScaler()
f_59=StandardScaler()
f_64=MinMaxScaler()
f_65=StandardScaler()
f_66=StandardScaler()
f_67=StandardScaler()
f_68=StandardScaler()
f_69=StandardScaler()
f_70=StandardScaler()
[2023-06-17 21:55:45] INFO 
Model Config: 

data:
	binarized_rating_thres=None
	fm_eval=False
	neg_count=0
	sampler=None
	shuffle=False
	split_mode=entry
	split_ratio=[3387880, 97972, 160973]
	fmeval=True
	low_rating_thres=None
eval:
	batch_size=4096
	cutoff=[5, 10, 20]
	val_metrics=['logloss', 'auc', 'accuracy', 'f1', 'precision', 'recall']
	val_n_epoch=1
	test_metrics=['logloss', 'auc', 'accuracy', 'f1', 'precision', 'recall']
	topk=100
	save_path=./saved/
	binarized_prob_thres=0.5
model:
	embed_dim=10
	item_bias=False
	cin_layer_size=[100, 100, 100]
	mlp_layer=[128, 128, 128]
	activation=relu
	dropout=0.2
	direct=False
train:
	accelerator=gpu
	ann=None
	batch_size=512
	early_stop_mode=min
	early_stop_patience=10
	epochs=1000
	gpu=[4]
	grad_clip_norm=None
	init_method=xavier_normal
	item_batch_size=1024
	learner=adam
	learning_rate=0.001
	num_threads=10
	sampling_method=none
	sampler=uniform
	negative_count=0
	excluding_hist=False
	scheduler=None
	seed=2022
	weight_decay=0.0
	tensorboard_path=None
[2023-06-17 21:55:45] INFO save_dir:./saved/
[2023-06-17 21:55:45] INFO xDeepFM(
  (loss_fn): BCEWithLogitLoss()
  (linear): LinearLayer(
    bias=True
    (embeddings): ModuleDict(
      (f_8): Embedding(7, 1, padding_idx=0)
      (f_12): Embedding(27, 1, padding_idx=0)
      (f_77): Embedding(5, 1, padding_idx=0)
      (f_37): Embedding(3, 1, padding_idx=0)
      (f_69): DenseEmbedding(
        embedding_dim=1, bias=False, batch_norm=True
        (batch_norm_layer): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (weight): Linear(in_features=1, out_features=1, bias=False)
      )
      (f_36): Embedding(3, 1, padding_idx=0)
      (f_35): Embedding(3, 1, padding_idx=0)
      (f_21): Embedding(36, 1, padding_idx=0)
      (f_32): Embedding(5, 1, padding_idx=0)
      (f_79): Embedding(8, 1, padding_idx=0)
      (f_47): Embedding(28, 1, padding_idx=0)
      (f_76): Embedding(9, 1, padding_idx=0)
      (f_71): Embedding(5, 1, padding_idx=0)
      (f_58): DenseEmbedding(
        embedding_dim=1, bias=False, batch_norm=True
        (batch_norm_layer): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (weight): Linear(in_features=1, out_features=1, bias=False)
      )
      (f_43): DenseEmbedding(
        embedding_dim=1, bias=False, batch_norm=True
        (batch_norm_layer): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (weight): Linear(in_features=1, out_features=1, bias=False)
      )
      (f_3): Embedding(6, 1, padding_idx=0)
      (f_26): Embedding(3, 1, padding_idx=0)
      (f_70): DenseEmbedding(
        embedding_dim=1, bias=False, batch_norm=True
        (batch_norm_layer): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (weight): Linear(in_features=1, out_features=1, bias=False)
      )
      (f_4): Embedding(639, 1, padding_idx=0)
      (f_16): Embedding(13, 1, padding_idx=0)
      (f_14): Embedding(20, 1, padding_idx=0)
      (f_60): Embedding(403, 1, padding_idx=0)
      (f_52): Embedding(172, 1, padding_idx=0)
      (f_33): Embedding(3, 1, padding_idx=0)
      (f_11): Embedding(25, 1, padding_idx=0)
      (f_39): Embedding(3, 1, padding_idx=0)
      (f_17): Embedding(50, 1, padding_idx=0)
      (f_44): Embedding(22, 1, padding_idx=0)
      (f_6): Embedding(5235, 1, padding_idx=0)
      (f_56): Embedding(221, 1, padding_idx=0)
      (f_54): Embedding(213, 1, padding_idx=0)
      (f_25): Embedding(4, 1, padding_idx=0)
      (f_78): Embedding(14, 1, padding_idx=0)
      (f_51): Embedding(1829, 1, padding_idx=0)
      (f_22): Embedding(27, 1, padding_idx=0)
      (f_64): DenseEmbedding(
        embedding_dim=1, bias=False, batch_norm=True
        (batch_norm_layer): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (weight): Linear(in_features=1, out_features=1, bias=False)
      )
      (f_34): Embedding(3, 1, padding_idx=0)
      (f_46): Embedding(12, 1, padding_idx=0)
      (f_63): Embedding(408, 1, padding_idx=0)
      (f_59): DenseEmbedding(
        embedding_dim=1, bias=False, batch_norm=True
        (batch_norm_layer): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (weight): Linear(in_features=1, out_features=1, bias=False)
      )
      (f_30): Embedding(3, 1, padding_idx=0)
      (f_19): Embedding(20, 1, padding_idx=0)
      (f_2): Embedding(140, 1, padding_idx=0)
      (f_61): Embedding(826, 1, padding_idx=0)
      (f_67): DenseEmbedding(
        embedding_dim=1, bias=False, batch_norm=True
        (batch_norm_layer): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (weight): Linear(in_features=1, out_features=1, bias=False)
      )
      (f_66): DenseEmbedding(
        embedding_dim=1, bias=False, batch_norm=True
        (batch_norm_layer): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (weight): Linear(in_features=1, out_features=1, bias=False)
      )
      (f_49): Embedding(21, 1, padding_idx=0)
      (f_24): Embedding(5, 1, padding_idx=0)
      (f_62): Embedding(1397, 1, padding_idx=0)
      (f_74): Embedding(5, 1, padding_idx=0)
      (f_55): Embedding(390, 1, padding_idx=0)
      (f_42): Embedding(8883, 1, padding_idx=0)
      (f_13): Embedding(332, 1, padding_idx=0)
      (f_45): Embedding(24, 1, padding_idx=0)
      (f_40): Embedding(3, 1, padding_idx=0)
      (f_20): Embedding(58, 1, padding_idx=0)
      (f_57): Embedding(517, 1, padding_idx=0)
      (f_41): Embedding(3, 1, padding_idx=0)
      (f_5): Embedding(7, 1, padding_idx=0)
      (f_65): DenseEmbedding(
        embedding_dim=1, bias=False, batch_norm=True
        (batch_norm_layer): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (weight): Linear(in_features=1, out_features=1, bias=False)
      )
      (f_72): Embedding(12, 1, padding_idx=0)
      (f_48): Embedding(28, 1, padding_idx=0)
      (f_50): Embedding(35, 1, padding_idx=0)
      (f_9): Embedding(8, 1, padding_idx=0)
      (f_23): Embedding(5, 1, padding_idx=0)
      (f_73): Embedding(9, 1, padding_idx=0)
      (f_75): Embedding(32, 1, padding_idx=0)
      (f_68): DenseEmbedding(
        embedding_dim=1, bias=False, batch_norm=True
        (batch_norm_layer): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (weight): Linear(in_features=1, out_features=1, bias=False)
      )
      (f_53): Embedding(103, 1, padding_idx=0)
      (f_31): Embedding(3, 1, padding_idx=0)
      (f_10): Embedding(4, 1, padding_idx=0)
      (f_15): Embedding(5855, 1, padding_idx=0)
      (f_18): Embedding(925, 1, padding_idx=0)
      (f_38): Embedding(3, 1, padding_idx=0)
      (f_1): DenseEmbedding(
        embedding_dim=1, bias=False, batch_norm=True
        (batch_norm_layer): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (weight): Linear(in_features=1, out_features=1, bias=False)
      )
    )
  )
  (fm): FMLayer(reduction=sum)
  (embedding): Embeddings(
    num_features=75, embed_dim=10, reduction=mean
    (embeddings): ModuleDict(
      (f_8): Embedding(7, 10, padding_idx=0)
      (f_12): Embedding(27, 10, padding_idx=0)
      (f_77): Embedding(5, 10, padding_idx=0)
      (f_37): Embedding(3, 10, padding_idx=0)
      (f_69): DenseEmbedding(
        embedding_dim=10, bias=False, batch_norm=True
        (batch_norm_layer): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (weight): Linear(in_features=1, out_features=10, bias=False)
      )
      (f_36): Embedding(3, 10, padding_idx=0)
      (f_35): Embedding(3, 10, padding_idx=0)
      (f_21): Embedding(36, 10, padding_idx=0)
      (f_32): Embedding(5, 10, padding_idx=0)
      (f_79): Embedding(8, 10, padding_idx=0)
      (f_47): Embedding(28, 10, padding_idx=0)
      (f_76): Embedding(9, 10, padding_idx=0)
      (f_71): Embedding(5, 10, padding_idx=0)
      (f_58): DenseEmbedding(
        embedding_dim=10, bias=False, batch_norm=True
        (batch_norm_layer): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (weight): Linear(in_features=1, out_features=10, bias=False)
      )
      (f_43): DenseEmbedding(
        embedding_dim=10, bias=False, batch_norm=True
        (batch_norm_layer): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (weight): Linear(in_features=1, out_features=10, bias=False)
      )
      (f_3): Embedding(6, 10, padding_idx=0)
      (f_26): Embedding(3, 10, padding_idx=0)
      (f_70): DenseEmbedding(
        embedding_dim=10, bias=False, batch_norm=True
        (batch_norm_layer): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (weight): Linear(in_features=1, out_features=10, bias=False)
      )
      (f_4): Embedding(639, 10, padding_idx=0)
      (f_16): Embedding(13, 10, padding_idx=0)
      (f_14): Embedding(20, 10, padding_idx=0)
      (f_60): Embedding(403, 10, padding_idx=0)
      (f_52): Embedding(172, 10, padding_idx=0)
      (f_33): Embedding(3, 10, padding_idx=0)
      (f_11): Embedding(25, 10, padding_idx=0)
      (f_39): Embedding(3, 10, padding_idx=0)
      (f_17): Embedding(50, 10, padding_idx=0)
      (f_44): Embedding(22, 10, padding_idx=0)
      (f_6): Embedding(5235, 10, padding_idx=0)
      (f_56): Embedding(221, 10, padding_idx=0)
      (f_54): Embedding(213, 10, padding_idx=0)
      (f_25): Embedding(4, 10, padding_idx=0)
      (f_78): Embedding(14, 10, padding_idx=0)
      (f_51): Embedding(1829, 10, padding_idx=0)
      (f_22): Embedding(27, 10, padding_idx=0)
      (f_64): DenseEmbedding(
        embedding_dim=10, bias=False, batch_norm=True
        (batch_norm_layer): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (weight): Linear(in_features=1, out_features=10, bias=False)
      )
      (f_34): Embedding(3, 10, padding_idx=0)
      (f_46): Embedding(12, 10, padding_idx=0)
      (f_63): Embedding(408, 10, padding_idx=0)
      (f_59): DenseEmbedding(
        embedding_dim=10, bias=False, batch_norm=True
        (batch_norm_layer): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (weight): Linear(in_features=1, out_features=10, bias=False)
      )
      (f_30): Embedding(3, 10, padding_idx=0)
      (f_19): Embedding(20, 10, padding_idx=0)
      (f_2): Embedding(140, 10, padding_idx=0)
      (f_61): Embedding(826, 10, padding_idx=0)
      (f_67): DenseEmbedding(
        embedding_dim=10, bias=False, batch_norm=True
        (batch_norm_layer): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (weight): Linear(in_features=1, out_features=10, bias=False)
      )
      (f_66): DenseEmbedding(
        embedding_dim=10, bias=False, batch_norm=True
        (batch_norm_layer): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (weight): Linear(in_features=1, out_features=10, bias=False)
      )
      (f_49): Embedding(21, 10, padding_idx=0)
      (f_24): Embedding(5, 10, padding_idx=0)
      (f_62): Embedding(1397, 10, padding_idx=0)
      (f_74): Embedding(5, 10, padding_idx=0)
      (f_55): Embedding(390, 10, padding_idx=0)
      (f_42): Embedding(8883, 10, padding_idx=0)
      (f_13): Embedding(332, 10, padding_idx=0)
      (f_45): Embedding(24, 10, padding_idx=0)
      (f_40): Embedding(3, 10, padding_idx=0)
      (f_20): Embedding(58, 10, padding_idx=0)
      (f_57): Embedding(517, 10, padding_idx=0)
      (f_41): Embedding(3, 10, padding_idx=0)
      (f_5): Embedding(7, 10, padding_idx=0)
      (f_65): DenseEmbedding(
        embedding_dim=10, bias=False, batch_norm=True
        (batch_norm_layer): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (weight): Linear(in_features=1, out_features=10, bias=False)
      )
      (f_72): Embedding(12, 10, padding_idx=0)
      (f_48): Embedding(28, 10, padding_idx=0)
      (f_50): Embedding(35, 10, padding_idx=0)
      (f_9): Embedding(8, 10, padding_idx=0)
      (f_23): Embedding(5, 10, padding_idx=0)
      (f_73): Embedding(9, 10, padding_idx=0)
      (f_75): Embedding(32, 10, padding_idx=0)
      (f_68): DenseEmbedding(
        embedding_dim=10, bias=False, batch_norm=True
        (batch_norm_layer): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (weight): Linear(in_features=1, out_features=10, bias=False)
      )
      (f_53): Embedding(103, 10, padding_idx=0)
      (f_31): Embedding(3, 10, padding_idx=0)
      (f_10): Embedding(4, 10, padding_idx=0)
      (f_15): Embedding(5855, 10, padding_idx=0)
      (f_18): Embedding(925, 10, padding_idx=0)
      (f_38): Embedding(3, 10, padding_idx=0)
      (f_1): DenseEmbedding(
        embedding_dim=10, bias=False, batch_norm=True
        (batch_norm_layer): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (weight): Linear(in_features=1, out_features=10, bias=False)
      )
    )
  )
  (cin): CIN(
    (activation): ReLU()
    (weight): ModuleList()
    (weight_list): ModuleList(
      (0): Conv1d(5625, 100, kernel_size=(1,), stride=(1,))
      (1-2): 2 x Conv1d(3750, 100, kernel_size=(1,), stride=(1,))
    )
    (linear): Linear(in_features=200, out_features=1, bias=True)
  )
  (mlp): MLPModule(
    (model): Sequential(
      (0): Dropout(p=0.2, inplace=False)
      (1): Linear(in_features=750, out_features=128, bias=True)
      (2): ReLU()
      (3): Dropout(p=0.2, inplace=False)
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Dropout(p=0.2, inplace=False)
      (7): Linear(in_features=128, out_features=128, bias=True)
      (8): ReLU()
      (9): Dropout(p=0.2, inplace=False)
      (10): Linear(in_features=128, out_features=1, bias=True)
    )
  )
)
[2023-06-17 21:55:45] INFO GPU id [4] are selected.
[2023-06-17 21:59:32] INFO Training: Epoch=  0 [logloss=0.3764 accuracy=0.8252 f1=0.3460 precision=0.5787 recall=0.2469 auc=0.8191 train_loss_0=0.2726]
[2023-06-17 21:59:32] INFO Train time: 224.85989s. Valid time: 0.75394s. GPU RAM: 1.54/10.76 GB
[2023-06-17 21:59:32] INFO logloss improved. Best value: 0.3764
[2023-06-17 22:03:13] INFO Training: Epoch=  1 [logloss=0.3734 accuracy=0.8281 f1=0.3166 precision=0.6209 recall=0.2125 auc=0.8306 train_loss_0=0.2541]
[2023-06-17 22:03:13] INFO Train time: 220.41028s. Valid time: 0.71156s. GPU RAM: 1.54/10.76 GB
[2023-06-17 22:03:13] INFO logloss improved. Best value: 0.3734
[2023-06-17 22:06:53] INFO Training: Epoch=  2 [logloss=0.3717 accuracy=0.8281 f1=0.3272 precision=0.6140 recall=0.2230 auc=0.8288 train_loss_0=0.2474]
[2023-06-17 22:06:53] INFO Train time: 219.11705s. Valid time: 0.69967s. GPU RAM: 1.54/10.76 GB
[2023-06-17 22:06:53] INFO logloss improved. Best value: 0.3717
[2023-06-17 22:10:35] INFO Training: Epoch=  3 [logloss=0.3912 accuracy=0.8240 f1=0.2493 precision=0.6223 recall=0.1560 auc=0.8232 train_loss_0=0.2420]
[2023-06-17 22:10:35] INFO Train time: 221.15795s. Valid time: 0.69196s. GPU RAM: 1.54/10.76 GB
[2023-06-17 22:14:15] INFO Training: Epoch=  4 [logloss=0.3872 accuracy=0.8258 f1=0.2864 precision=0.6168 recall=0.1866 auc=0.8260 train_loss_0=0.2364]
[2023-06-17 22:14:15] INFO Train time: 219.74900s. Valid time: 0.70426s. GPU RAM: 1.54/10.76 GB
[2023-06-17 22:17:56] INFO Training: Epoch=  5 [logloss=0.3910 accuracy=0.8222 f1=0.3576 precision=0.5537 recall=0.2642 auc=0.8173 train_loss_0=0.2300]
[2023-06-17 22:17:56] INFO Train time: 219.75289s. Valid time: 0.70241s. GPU RAM: 1.54/10.76 GB
[2023-06-17 22:21:38] INFO Training: Epoch=  6 [logloss=0.4119 accuracy=0.8217 f1=0.3022 precision=0.5665 recall=0.2062 auc=0.8126 train_loss_0=0.2229]
[2023-06-17 22:21:38] INFO Train time: 221.28239s. Valid time: 0.70886s. GPU RAM: 1.54/10.76 GB
[2023-06-17 22:25:17] INFO Training: Epoch=  7 [logloss=0.4309 accuracy=0.8193 f1=0.2993 precision=0.5475 recall=0.2060 auc=0.8053 train_loss_0=0.2151]
[2023-06-17 22:25:17] INFO Train time: 219.02297s. Valid time: 0.69663s. GPU RAM: 1.54/10.76 GB
[2023-06-17 22:28:59] INFO Training: Epoch=  8 [logloss=0.4678 accuracy=0.8177 f1=0.2575 precision=0.5435 recall=0.1688 auc=0.8019 train_loss_0=0.2073]
[2023-06-17 22:28:59] INFO Train time: 221.22559s. Valid time: 0.68136s. GPU RAM: 1.54/10.76 GB
[2023-06-17 22:32:38] INFO Training: Epoch=  9 [logloss=0.4830 accuracy=0.8148 f1=0.2966 precision=0.5145 recall=0.2085 auc=0.7935 train_loss_0=0.1995]
[2023-06-17 22:32:38] INFO Train time: 218.38924s. Valid time: 0.68057s. GPU RAM: 1.54/10.76 GB
[2023-06-17 22:36:17] INFO Training: Epoch= 10 [logloss=0.4913 accuracy=0.8113 f1=0.3309 precision=0.4927 recall=0.2493 auc=0.7877 train_loss_0=0.1921]
[2023-06-17 22:36:17] INFO Train time: 218.04065s. Valid time: 0.71000s. GPU RAM: 1.54/10.76 GB
[2023-06-17 22:39:55] INFO Training: Epoch= 11 [logloss=0.5344 accuracy=0.8094 f1=0.3071 precision=0.4817 recall=0.2256 auc=0.7815 train_loss_0=0.1850]
[2023-06-17 22:39:55] INFO Train time: 216.97925s. Valid time: 0.69108s. GPU RAM: 1.54/10.76 GB
[2023-06-17 22:43:31] INFO Training: Epoch= 12 [logloss=0.5521 accuracy=0.8033 f1=0.3712 precision=0.4629 recall=0.3100 auc=0.7776 train_loss_0=0.1786]
[2023-06-17 22:43:31] INFO Train time: 215.67625s. Valid time: 0.68984s. GPU RAM: 1.54/10.76 GB
[2023-06-17 22:43:31] INFO Early stopped. Since the metric logloss haven't been improved for 10 epochs.
[2023-06-17 22:43:31] INFO The best score of logloss is 0.3717 on epoch 2
[2023-06-17 22:43:31] INFO Best model checkpoint saved in ./saved/xDeepFM/recsys/2023-06-17-21-55-43.ckpt.
[2023-06-17 22:43:58] INFO Predictions saved in ./predictions/xDeepFM/2023-06-17-22-43-31is_installed.csv
